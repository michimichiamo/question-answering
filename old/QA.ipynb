{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c69e289",
      "metadata": {
        "id": "3c69e289"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "291d7474",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-04T17:53:45.179473Z",
          "start_time": "2022-01-04T17:53:45.101515Z"
        },
        "id": "291d7474"
      },
      "outputs": [],
      "source": [
        "# Suppress output\n",
        "%%capture\n",
        "\n",
        "# Whether the notebook is run within Google Colab or not\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# General imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "# Install needed dependencies on Colab\n",
        "if colab:\n",
        "    !pip install transformers\n",
        "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
        "\n",
        "# Enable GPU acceleration, whenever available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Automatically reimport modules at each execution\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa58304",
      "metadata": {
        "id": "eaa58304"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3124ea1c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-28T18:25:38.678408Z",
          "start_time": "2021-12-28T18:25:37.388290Z"
        },
        "id": "3124ea1c"
      },
      "outputs": [],
      "source": [
        "# Execute this only to load the dataset in csv format if not already done\n",
        "# from read_dataset import read_dataset\n",
        "\n",
        "# dataset = read_dataset(path='training_set.json', validation_set_perc=20)\n",
        "# train_df = pd.DataFrame(dataset[0], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# train_df.to_csv('train_df.csv')\n",
        "# val_df = pd.DataFrame(dataset[1], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# val_df.to_csv('val_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8a453903",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-04T16:37:29.578980Z",
          "start_time": "2022-01-04T16:37:28.611162Z"
        },
        "id": "8a453903"
      },
      "outputs": [],
      "source": [
        "train_filename = './data/train_df.csv' if not colab else 'https://raw.githubusercontent.com/michimichiamo/question-answering/main/data/Dataset/train_df.csv?token=GHSAT0AAAAAABKAIOLOH645DE75GJJWC3WWYPG6MIA'\n",
        "val_filename = './data/val_df.csv' if not colab else 'https://raw.githubusercontent.com/michimichiamo/question-answering/main/data/Dataset/val_df.csv?token=GHSAT0AAAAAABKAIOLPB2NZWHYR4JBIK37IYPG6MIQ'\n",
        "\n",
        "train_df = pd.read_csv(train_filename)\n",
        "val_df = pd.read_csv(val_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6c7b87",
      "metadata": {
        "id": "ef6c7b87"
      },
      "source": [
        "### Read questions and contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8a4421e3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-04T16:37:31.515879Z",
          "start_time": "2022-01-04T16:37:31.480418Z"
        },
        "id": "8a4421e3",
        "outputId": "3941a7e6-0d90-4002-d903-f3aef9b08a7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "context_id: 0\n",
            "answer: (515, 541)\n",
            "question: What happened to Joseph I in 1758?\n",
            "context: Following the earthquake, Joseph I gave his Prime Minister even more power, and Sebastião de Melo became a powerful, progressive dictator. As his power grew, his enemies increased in number, and bitter disputes with the high nobility became frequent. In 1758 Joseph I was wounded in an attempted assassination. The Távora family and the Duke of Aveiro were implicated and executed after a quick trial. The Jesuits were expelled from the country and their assets confiscated by the crown. Sebastião de Melo prosecuted every person involved, even women and children. This was the final stroke that broke the power of the aristocracy. Joseph I made his loyal minister Count of Oeiras in 1759.\n",
            "context_id: 922\n",
            "answer: (272, 309)\n"
          ]
        }
      ],
      "source": [
        "train_questions = list(train_df['question'].values)\n",
        "train_contexts = list(train_df['context'].values)\n",
        "train_context_ids = list(train_df['context_id'].values)\n",
        "train_answers = [train_df['start'].values, train_df['end'].values]\n",
        "\n",
        "print('question:', train_questions[0])\n",
        "print('context:', train_contexts[0])\n",
        "print('context_id:', train_context_ids[0])\n",
        "print(f'answer: ({train_answers[0][0]}, {train_answers[1][0]})')\n",
        "\n",
        "val_questions = list(val_df['question'].values)\n",
        "val_contexts = list(val_df['context'].values)\n",
        "val_context_ids = list(val_df['context_id'].values)\n",
        "val_answers = [val_df['start'].values, val_df['end'].values]\n",
        "\n",
        "print('question:', val_questions[0])\n",
        "print('context:', val_contexts[0])\n",
        "print('context_id:', val_context_ids[0])\n",
        "print(f'answer: ({val_answers[0][0]}, {val_answers[1][0]})')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd6f886",
      "metadata": {
        "id": "0bd6f886"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "862509a4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-04T17:23:02.618090Z",
          "start_time": "2022-01-04T17:22:54.915789Z"
        },
        "id": "862509a4"
      },
      "outputs": [],
      "source": [
        "## Load tokenizer and transformers\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9a0fdafe",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-04T16:49:27.772624Z",
          "start_time": "2022-01-04T16:49:27.670794Z"
        },
        "code_folding": [],
        "id": "9a0fdafe",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "## Tokenize questions and contexts\n",
        "max_length = model.config.max_position_embeddings\n",
        "doc_stride = 256\n",
        "\n",
        "train_tokenized = tokenizer(\n",
        "   train_questions,\n",
        "   train_contexts,\n",
        "   max_length=max_length,\n",
        "   truncation=\"only_second\",\n",
        "   return_overflowing_tokens=True,\n",
        "   return_offsets_mapping=True,\n",
        "   stride=doc_stride,\n",
        "   return_attention_mask=True,\n",
        "   padding='max_length'\n",
        ")\n",
        "\n",
        "val_tokenized = tokenizer(\n",
        "   val_questions,\n",
        "   val_contexts,\n",
        "   max_length=max_length,\n",
        "   truncation=\"only_second\",\n",
        "   return_overflowing_tokens=True,\n",
        "   return_offsets_mapping=True,\n",
        "   stride=doc_stride,\n",
        "   return_attention_mask=True,\n",
        "   padding='max_length'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def move_answers_position(tokenized, answers):\n",
        "\n",
        "\tinput_ids = tokenized['input_ids']\n",
        "\toffsets = tokenized['offset_mapping']\n",
        "\tmappings = tokenized['overflow_to_sample_mapping']\n",
        "\n",
        "\tnew_answers = []\n",
        "\tall_inclusive = 0\n",
        "\tnot_all_inclusive = 0\n",
        "\tstart_inclusive = 0\n",
        "\tend_inclusive = 0\n",
        "\tfor post_index, pre_index in enumerate(mappings):\n",
        "\t\tstart_found = False\n",
        "\t\tend_found = False\n",
        "\t\tstart_index_word = 0\n",
        "\t\tend_index_word = 0\n",
        "\t\tans_start = answers[0][pre_index]\n",
        "\t\tans_end = answers[1][pre_index]\n",
        "\t\toffset = offsets[post_index]\n",
        "\t\tinput_id = input_ids[post_index]\n",
        "\n",
        "\t\t# the search starts after the question, so after the first 102 tag\n",
        "\t\tfor i in range(input_id.index(102), len(offset)):\n",
        "\t\t\tof = offset[i]\n",
        "\t\t\trange_offset = list(range(of[0], of[1]+1))\n",
        "\n",
        "\t\t\tif ans_start in range_offset:\n",
        "\t\t\t\t# save the word index i as the answer start\n",
        "\t\t\t\tstart_found = True\n",
        "\t\t\t\tstart_index_word = i\n",
        "\n",
        "\t\t\tif ans_end in range_offset:\n",
        "\t\t\t\t# save the word index i as the answer end\n",
        "\t\t\t\tend_found = True\n",
        "\t\t\t\tend_index_word = i\n",
        "\n",
        "\t\t\tif start_found and end_found:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t# the answer is completely included in this context -> no changes\n",
        "\t\tif start_found and end_found:\n",
        "\t\t\tall_inclusive += 1\n",
        "\t\t\tnew_answers.append((start_index_word, end_index_word))\n",
        "\t\t\tcontinue\n",
        "\t\telse:\n",
        "\t\t\t# here's the problems\n",
        "\t\t\tnot_all_inclusive += 1\n",
        "\t\t\tnew_answers.append((0, 0))\n",
        "\t\t\tcontinue\n",
        "\n",
        "\tprint(\"stats: (all, noth): {}\".format((all_inclusive, not_all_inclusive)))\n",
        "\treturn new_answers\n"
      ],
      "metadata": {
        "id": "lTrUoAu8QNHC"
      },
      "id": "lTrUoAu8QNHC",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_new_answers = move_answers_position(train_tokenized, train_answers)\n",
        "val_new_answers = move_answers_position(val_tokenized, val_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpgmEj9qSqUy",
        "outputId": "569c79ae-233a-471b-a023-c49fc03a3138"
      },
      "id": "YpgmEj9qSqUy",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stats: (all, noth): (71270, 84)\n",
            "stats: (all, noth): (16376, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_new_start, train_new_end = list(zip(*train_new_answers))\n",
        "val_new_start, val_new_end = list(zip(*val_new_answers))"
      ],
      "metadata": {
        "id": "0-iUaphoTShd"
      },
      "id": "0-iUaphoTShd",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_df = pd.DataFrame()\n",
        "new_train_df['input_ids'] = train_tokenized['input_ids']\n",
        "new_train_df['start'] = train_new_start\n",
        "new_train_df['end'] = train_new_end\n",
        "\n",
        "new_val_df = pd.DataFrame()\n",
        "new_val_df['input_ids'] = val_tokenized['input_ids']\n",
        "new_val_df['start'] = val_new_start\n",
        "new_val_df['end'] = val_new_end"
      ],
      "metadata": {
        "id": "_hfIPpGgbxiB"
      },
      "id": "_hfIPpGgbxiB",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_df.to_csv('pp_train_df.csv')"
      ],
      "metadata": {
        "id": "H0o3bNemkass"
      },
      "id": "H0o3bNemkass",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_val_df.to_csv('pp_val_df.csv')"
      ],
      "metadata": {
        "id": "BOuAaNm4klXg"
      },
      "id": "BOuAaNm4klXg",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Move to device\n",
        "#bert_dict = {}\n",
        "#\n",
        "#bert_dict['input_ids'] = torch.IntTensor(tokenized['input_ids']).to(device)\n",
        "#bert_dict['attention_mask'] = torch.IntTensor(tokenized['attention_mask']).to(device)\n",
        "#\n",
        "#model = model.to(device)"
      ],
      "metadata": {
        "id": "f0Bc1VhcCiQF"
      },
      "id": "f0Bc1VhcCiQF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Network structure\n",
        "\n",
        "#transformed = model(**bert_dict)\n",
        "#dropped = torch.nn.Dropout(0.3)(transformed[0])\n",
        "#logits = torch.nn.Linear(768, 2, device=device)(dropped)\n",
        "#start_logits, end_logits = logits.split(1, dim=-1)\n",
        "#start_logits = start_logits.squeeze(-1)\n",
        "#end_logits = end_logits.squeeze(-1)\n",
        "#outputs = (start_logits, end_logits)"
      ],
      "metadata": {
        "id": "ldtj3wxmCU9b"
      },
      "id": "ldtj3wxmCU9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd5836ff",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-23T17:23:09.025496Z",
          "start_time": "2021-12-23T17:23:08.944964Z"
        },
        "id": "bd5836ff"
      },
      "outputs": [],
      "source": [
        "#tokenizer.decode(tokenized['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a525d5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-04T17:38:27.953037Z",
          "start_time": "2022-01-04T17:38:27.765928Z"
        },
        "id": "04a525d5"
      },
      "outputs": [],
      "source": [
        "#train_df['question'].apply(lambda x: len(x.strip().split(' '))).max()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO"
      ],
      "metadata": {
        "id": "7QB6LO9WCpyO"
      },
      "id": "7QB6LO9WCpyO"
    },
    {
      "cell_type": "markdown",
      "id": "22c4d22e",
      "metadata": {
        "id": "22c4d22e"
      },
      "source": [
        "- `tokenized['offset_mapping'][0]` returna le tuple (start,end) di ogni parola dell'input (query, context)\n",
        "\n",
        "- Problema: splittare i contesti online (nel Dataloader) produce batch di lunghezza variabile\n",
        "    - Prima soluzione: eliminare gli split che non contengono la domanda\n",
        "    - Seconda soluzione: creare dataframe con contesti già splittati usando il tokenizer (lunghezza 512, overlapping 256) invece che farlo online nel Dataloader\n",
        "\n",
        "- Problema: risposte sono presenti in un solo split, cosa fare con gli altri?\n",
        "    - Una [soluzione](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=iLekL6Un9D70&line=24&uniqifier=1): riscalare tuple di contesti tagliati (invece che 0, allinearli alla risposta)\n",
        "\n",
        "    - Un'altra soluzione: \n",
        "        - riscalare answer_start e answer_end per ogni contesto\n",
        "        - lo split che contiene la risposta mantiene answer_start e answer_end, gli altri split dello stesso contesto vanno trattati (assegniamo (0,0)? oppure scartiamo)\n",
        "        - Possibile [soluzione](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=v86c_RApFdNG) (in grassetto parte interessante): *Now let's put everything together in one function we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), **we set the cls index for both the start and end position**. We could also simply discard those examples from the training set if the flag allow_impossible_answers is False. Since the preprocessing is already complex enough as it is, we've kept is simple for this part.*\n",
        "\n",
        "- N.B. sul token `[CLS]` preso da [qui](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=kv1iD9E6FdND):\n",
        "> *The very first token ([CLS]) has (0, 0) because it doesn't correspond to any part of the question/answer, then the second token is the same as the characters 0 to 3 of the question*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6205600e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-04T17:10:05.826417Z",
          "start_time": "2022-01-04T17:10:05.741367Z"
        },
        "id": "6205600e"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4916e6fc",
      "metadata": {
        "id": "4916e6fc"
      },
      "outputs": [],
      "source": [
        "class QA(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=768, num_labels=2, dropout_rate=0.5):\n",
        "        super(QA, self).__init__()\n",
        "        # Device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        # Parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        # Layers\n",
        "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "        self.transformers = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad').to(self.device)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        #self.extra_linear = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        #self.extra_linear_tanh = torch.nn.Tanh()\n",
        "        self.dense = torch.nn.Linear(self.hidden_size, self.num_labels, device=self.device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Unpack inputs\n",
        "        questions, contexts = inputs\n",
        "        # Tokenizer\n",
        "        max_length = self.transformers.config.max_position_embeddings\n",
        "        doc_stride = 128\n",
        "        tokenized = self.tokenizer(\n",
        "            questions,\n",
        "            contexts,\n",
        "            max_length=max_length,\n",
        "            truncation=\"only_second\",\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            stride=doc_stride,\n",
        "            return_attention_mask=True,\n",
        "            padding='max_length'\n",
        "        )\n",
        "        # Put to device\n",
        "        bert_dict = {}\n",
        "\n",
        "        bert_dict['input_ids'] = torch.IntTensor(tokenized['input_ids']).to(self.device)\n",
        "        bert_dict['attention_mask'] = torch.IntTensor(tokenized['attention_mask']).to(self.device)\n",
        "        # Transformers \n",
        "        transformed = self.transformers(**bert_dict)\n",
        "        # Dropout\n",
        "        dropped = self.dropout(transformed[0])\n",
        "        # Obtain logits\n",
        "        logits = self.dense(dropped) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
        "        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
        "        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
        "        # --- 4) Prepare output tuple\n",
        "        outputs = (start_logits, end_logits)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = QA()"
      ],
      "metadata": {
        "id": "cFqtEIvq9bio"
      },
      "id": "cFqtEIvq9bio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = (questions, contexts)"
      ],
      "metadata": {
        "id": "ZGPcL43o9eUO"
      },
      "id": "ZGPcL43o9eUO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, questions, contexts):\n",
        "        'Initialization'\n",
        "        self.questions = questions\n",
        "        self.contexts = contexts\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.questions)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        question = self.questions[index]\n",
        "        context = self.contexts[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        X = (question, context)\n",
        "        #y = answer\n",
        "\n",
        "        return X#, y"
      ],
      "metadata": {
        "id": "ZX7lp9lGApN4"
      },
      "id": "ZX7lp9lGApN4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset(questions, contexts)"
      ],
      "metadata": {
        "id": "QqhlR1A9BKsa"
      },
      "id": "QqhlR1A9BKsa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.utils.data.DataLoader(data, batch_size=32, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "bPt21jd5-ZD9"
      },
      "id": "bPt21jd5-ZD9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs in generator:\n",
        "    print(inputs)\n",
        "    outputs = net.forward(inputs)\n",
        "    break"
      ],
      "metadata": {
        "id": "dhvSBZ549wLG"
      },
      "id": "dhvSBZ549wLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "bNHsFM8BAZn2"
      },
      "id": "bNHsFM8BAZn2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hCy5-Ch5CIUp"
      },
      "id": "hCy5-Ch5CIUp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "QA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}