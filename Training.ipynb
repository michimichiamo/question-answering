{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c69e289",
      "metadata": {
        "id": "3c69e289"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "291d7474",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.154416Z",
          "start_time": "2022-01-12T12:45:57.538962Z"
        },
        "id": "291d7474"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppress output\n",
        "\n",
        "# Whether the notebook is run within Google Colab or not\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "# Install needed dependencies on Colab\n",
        "if colab:\n",
        "    !pip install transformers\n",
        "from transformers import DistilBertModel#, DistilBertTokenizerFast\n",
        "\n",
        "# Enable GPU acceleration, whenever available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Automatically reimport modules at each execution\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86992ec3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.223020Z",
          "start_time": "2022-01-12T12:46:05.157398Z"
        },
        "id": "86992ec3",
        "outputId": "df90a4b8-cec8-44d6-94dc-2d93505013b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'question-answering' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "if colab:\n",
        "    !git clone 'https://github.com/michimichiamo/question-answering'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa58304",
      "metadata": {
        "id": "eaa58304"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3124ea1c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.281868Z",
          "start_time": "2022-01-12T12:46:05.226393Z"
        },
        "id": "3124ea1c"
      },
      "outputs": [],
      "source": [
        "# Execute this only to load the dataset in csv format if not already done\n",
        "# from read_dataset import read_dataset\n",
        "\n",
        "# dataset = read_dataset(path='training_set.json', validation_set_perc=20)\n",
        "# train_df = pd.DataFrame(dataset[0], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# train_df.to_csv('train_df.csv')\n",
        "# val_df = pd.DataFrame(dataset[1], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# val_df.to_csv('val_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fb334c6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.344966Z",
          "start_time": "2022-01-12T12:46:05.284455Z"
        },
        "id": "6fb334c6"
      },
      "outputs": [],
      "source": [
        "directory='./' if not colab else './question-answering/'\n",
        "\n",
        "train_filename = directory+'data/tokenized/train.npz'\n",
        "#val_filename = directory+'data/tokenized/val.npz'\n",
        "\n",
        "train_data = np.load(train_filename)\n",
        "#val_data = np.load(val_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6b01bd4b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:07.116897Z",
          "start_time": "2022-01-12T12:46:05.347475Z"
        },
        "id": "6b01bd4b"
      },
      "outputs": [],
      "source": [
        "input_ids = train_data['input_ids']\n",
        "attention_mask = train_data['attention_mask']\n",
        "answer_start = train_data['answer_start']\n",
        "answer_end = train_data['answer_end']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6205600e",
      "metadata": {
        "id": "6205600e"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4916e6fc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:07.652998Z",
          "start_time": "2022-01-12T12:46:07.593116Z"
        },
        "id": "4916e6fc"
      },
      "outputs": [],
      "source": [
        "class QA(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=768, num_labels=2, dropout_rate=0.5):\n",
        "        super(QA, self).__init__()\n",
        "        # Device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        # Parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        # Layers\n",
        "        #self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "        self.transformers = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad').to(self.device)\n",
        "        self.transformers.requires_grad_(False)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        #self.extra_linear = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        #self.extra_linear_tanh = torch.nn.Tanh()\n",
        "        self.dense = torch.nn.Linear(self.hidden_size, self.num_labels, device=self.device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Unpack inputs\n",
        "        input_ids, attention_mask = inputs\n",
        "        \n",
        "        # Put to device\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        \n",
        "        # Transformers \n",
        "        transformed = self.transformers(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Dropout\n",
        "        dropped = self.dropout(transformed[0])\n",
        "        # Obtain logits\n",
        "        logits = self.dense(dropped) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
        "        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
        "        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
        "        # --- 4) Prepare output tuple\n",
        "        outputs = (start_logits, end_logits)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cFqtEIvq9bio",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.796206Z",
          "start_time": "2022-01-12T12:46:07.655319Z"
        },
        "id": "cFqtEIvq9bio",
        "outputId": "97e70bd0-31b8-4c3c-e312-c2d772c1a8c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QA(\n",
              "  (transformers): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (dense): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "net = QA()\n",
        "net.to(net.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ZX7lp9lGApN4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.864157Z",
          "start_time": "2022-01-12T12:46:10.798299Z"
        },
        "id": "ZX7lp9lGApN4"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, input_ids, attention_masks, answer_starts, answer_ends):\n",
        "        'Initialization'\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.answer_starts = answer_starts\n",
        "        self.answer_ends = answer_ends\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        input_id = self.input_ids[index]\n",
        "        attention_mask = self.attention_masks[index]\n",
        "        answer_start = self.answer_starts[index]\n",
        "        answer_end = self.answer_ends[index]\n",
        "\n",
        "        # Pack input and output\n",
        "        X = (input_id, attention_mask)\n",
        "        y = (answer_start, answer_end)\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "QqhlR1A9BKsa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.924544Z",
          "start_time": "2022-01-12T12:46:10.866537Z"
        },
        "id": "QqhlR1A9BKsa"
      },
      "outputs": [],
      "source": [
        "data = Dataset(input_ids, attention_mask, answer_start, answer_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bPt21jd5-ZD9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.983614Z",
          "start_time": "2022-01-12T12:46:10.927066Z"
        },
        "id": "bPt21jd5-ZD9"
      },
      "outputs": [],
      "source": [
        "dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size)#, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del input_ids, attention_mask, answer_start, answer_end\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "R-ME4IcRMhb2",
        "outputId": "13199f09-be26-47cf-e0d1-67ed8b71cca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "R-ME4IcRMhb2",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "488"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "#@title Hyperparameters\n",
        "batch_size = 32 #@param [\"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "learning_rate = 0.001 #@param [\"0.00001\", \"0.0001\", \"0.001\", \"0.01\", \"0.1\", \"1\"] {type:\"raw\"}\n",
        "epochs = 5 #@param {type:\"slider\", min:5, max:200, step:5}\n",
        "\n",
        "loss_fn = CrossEntropyLoss()\n",
        "optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "n_iter = round(len(dataloader)/(batch_size))"
      ],
      "metadata": {
        "id": "IZpQ_Tn6PnTC"
      },
      "id": "IZpQ_Tn6PnTC",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dhvSBZ549wLG",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T14:39:15.485889Z",
          "start_time": "2022-01-12T12:46:11.068018Z"
        },
        "id": "dhvSBZ549wLG",
        "outputId": "a99a1701-098c-4963-d1c6-f6c01f676522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0/69.6875: loss = 49588.09863547748\n",
            "iter 1/69.6875: loss = 51021.03443966964\n",
            "iter 2/69.6875: loss = 51907.26640214165\n",
            "iter 3/69.6875: loss = 51935.4375278352\n",
            "iter 4/69.6875: loss = 51964.82127585965\n",
            "iter 5/69.6875: loss = 51333.421006549295\n",
            "iter 6/69.6875: loss = 50412.28318748968\n",
            "iter 7/69.6875: loss = 48619.23014565387\n",
            "iter 8/69.6875: loss = 47881.48497888457\n"
          ]
        }
      ],
      "source": [
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for iter, (inputs, targets) in enumerate(dataloader):\n",
        "        # Unpack targets\n",
        "        start_logits, end_logits = targets\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        start_out, end_out = net.forward(inputs)\n",
        "        # Loss function\n",
        "        loss = -loss_fn(start_logits, start_out) -loss_fn(end_logits, end_out)\n",
        "        # Gradient update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track loss\n",
        "        print(f'iter {iter}/{n_iter}: loss = {loss}')\n",
        "        loss_history.append(loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "QA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}