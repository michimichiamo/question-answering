{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c69e289",
      "metadata": {
        "id": "3c69e289"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291d7474",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:24.250793Z",
          "start_time": "2022-01-25T16:20:22.231127Z"
        },
        "id": "291d7474"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppress output\n",
        "\n",
        "# Whether the notebook is run within Google Colab or not\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sys\n",
        "from numpy.core.numeric import full\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "\n",
        "# Install needed dependencies on Colab\n",
        "if colab:\n",
        "    !pip install transformers\n",
        "    !pip install torchmetrics==0.6\n",
        "#from transformers import DistilBertModel#, DistilBertTokenizerFast\n",
        "import gc\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "# Automatically reimport modules at each execution\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa58304",
      "metadata": {
        "id": "eaa58304"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86992ec3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:27.832601Z",
          "start_time": "2022-01-25T16:20:27.786158Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86992ec3",
        "outputId": "393fd7d9-9525-4c52-9b3c-2041584396c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'question-answering'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (388/388), done.\u001b[K\n",
            "remote: Compressing objects: 100% (324/324), done.\u001b[K\n",
            "remote: Total 401 (delta 214), reused 173 (delta 63), pack-reused 13\u001b[K\n",
            "Receiving objects: 100% (401/401), 134.42 MiB | 15.29 MiB/s, done.\n",
            "Resolving deltas: 100% (217/217), done.\n",
            "Checking out files: 100% (36/36), done.\n",
            "/content/question-answering\n"
          ]
        }
      ],
      "source": [
        "if colab:\n",
        "    !git clone 'https://github.com/michimichiamo/question-answering'\n",
        "    %cd /content/question-answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3124ea1c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:29.238997Z",
          "start_time": "2022-01-25T16:20:29.173790Z"
        },
        "code_folding": [
          0
        ],
        "id": "3124ea1c"
      },
      "outputs": [],
      "source": [
        "# Execute this only to load the dataset in csv format if not already done\n",
        "# from read_dataset import read_dataset\n",
        "\n",
        "# dataset = read_dataset(path='training_set.json', validation_set_perc=20)\n",
        "# train_df = pd.DataFrame(dataset[0], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# train_df.to_csv('train_df.csv')\n",
        "# val_df = pd.DataFrame(dataset[1], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# val_df.to_csv('val_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fb334c6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:33.046712Z",
          "start_time": "2022-01-25T16:20:30.193118Z"
        },
        "id": "6fb334c6",
        "outputId": "09d59d67-2006-4e8c-8455-6bbc1e09c56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-169b30b4a0bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtr_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/question-answering/util/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAveragePrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from util.model import read_npz\n",
        "tr_id, tr_input, tr_attention_mask, tr_start, tr_end = read_npz(split='train')\n",
        "val_id, val_input, val_attention_mask, val_start, val_end = read_npz(split='val')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6205600e",
      "metadata": {
        "id": "6205600e"
      },
      "source": [
        "## Prepare for training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S_BdX_aD6mDt",
      "metadata": {
        "id": "S_BdX_aD6mDt"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SFKL9c4TZSa3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:39.553600Z",
          "start_time": "2022-01-25T16:20:39.438447Z"
        },
        "id": "SFKL9c4TZSa3"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparameters\n",
        "batch_size = 256 #@param [\"32\", \"64\", \"128\", \"256\", \"512\"] {type:\"raw\"}\n",
        "learning_rate = 0.001 #@param [\"0.00001\", \"0.0001\", \"0.001\", \"0.01\", \"0.1\", \"1\"] {type:\"raw\"}\n",
        "epochs = 100 #@param {type:\"slider\", min:5, max:200, step:5}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lKiyGfam6rSG",
      "metadata": {
        "id": "lKiyGfam6rSG"
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QqhlR1A9BKsa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:40.354418Z",
          "start_time": "2022-01-25T16:20:40.296874Z"
        },
        "id": "QqhlR1A9BKsa"
      },
      "outputs": [],
      "source": [
        "from util.model import Dataset\n",
        "tr_dataset = Dataset(tr_id, tr_input, tr_attention_mask, tr_start, tr_end)\n",
        "tr_dataloader = torch.utils.data.DataLoader(tr_dataset, batch_size=batch_size, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arGKmyRmXSlB",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:41.168735Z",
          "start_time": "2022-01-25T16:20:41.110816Z"
        },
        "id": "arGKmyRmXSlB"
      },
      "outputs": [],
      "source": [
        "val_dataset = Dataset(val_id, val_input, val_attention_mask, val_start, val_end)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, pin_memory=True)\n",
        "val_dataloader = iter(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R-ME4IcRMhb2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:42.841492Z",
          "start_time": "2022-01-25T16:20:42.547308Z"
        },
        "id": "R-ME4IcRMhb2"
      },
      "outputs": [],
      "source": [
        "del tr_id, tr_input, tr_attention_mask, tr_start, tr_end\n",
        "del val_id, val_input, val_attention_mask, val_start, val_end\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ni6qjrZb6vF0",
      "metadata": {
        "id": "Ni6qjrZb6vF0"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IZpQ_Tn6PnTC",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:49.279113Z",
          "start_time": "2022-01-25T16:20:45.127866Z"
        },
        "id": "IZpQ_Tn6PnTC"
      },
      "outputs": [],
      "source": [
        "from util.model import QA, evaluate, define_metrics\n",
        "\n",
        "# Create model\n",
        "net = QA()\n",
        "net.to(net.device)\n",
        "# Optimizer, loss function\n",
        "optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "#scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "n_iter = len(tr_dataloader)\n",
        "# Metrics\n",
        "metrics = define_metrics(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9xZJ_ImI6yqC",
      "metadata": {
        "id": "9xZJ_ImI6yqC"
      },
      "source": [
        "### Setup to store log and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ulS98qb2GOh2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:53.542845Z",
          "start_time": "2022-01-25T16:20:53.479486Z"
        },
        "id": "ulS98qb2GOh2"
      },
      "outputs": [],
      "source": [
        "homedir = None\n",
        "if colab:\n",
        "    # Setting Google Drive for Colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    homedir = '/content/gdrive/My Drive/QA project/'\n",
        "else:\n",
        "    homedir = './data/logs/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Log paths\n",
        "\n",
        "#Creation of the dirs to store weights/logs\n",
        "utc_string = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
        "#Result dir\n",
        "if not os.path.isdir(homedir+utc_string):\n",
        "    os.mkdir(homedir+utc_string)\n",
        "res_dir = homedir+utc_string\n",
        "#Tensorboard dir (runs)\n",
        "if not os.path.isdir(res_dir+'/runs/'):\n",
        "    os.mkdir(res_dir+'/runs/')\n",
        "runs_dir = res_dir+'/runs/'\n",
        "#Logs dir\n",
        "if not os.path.isdir(res_dir+'/logs/'):\n",
        "    os.mkdir(res_dir+'/logs/')\n",
        "logs_dir = res_dir+'/logs/'\n",
        "#Checkpoints dir\n",
        "if not os.path.isdir(res_dir+'/checkpoints/'):\n",
        "    os.mkdir(res_dir+'/checkpoints/')\n",
        "checkpoints_dir = res_dir+'/checkpoints/'\n",
        "\n",
        "#Tensorboard init\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(runs_dir)"
      ],
      "metadata": {
        "id": "8k5F9TdrjKyo"
      },
      "id": "8k5F9TdrjKyo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "xxN_nP9_68zq",
      "metadata": {
        "id": "xxN_nP9_68zq"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Ry3m40m8alP",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:20:58.540405Z",
          "start_time": "2022-01-25T16:20:54.329411Z"
        },
        "id": "3Ry3m40m8alP"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"$runs_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dhvSBZ549wLG",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-25T16:21:06.621655Z",
          "start_time": "2022-01-25T16:21:06.194839Z"
        },
        "id": "dhvSBZ549wLG"
      },
      "outputs": [],
      "source": [
        "#Training function\n",
        "def train(model, save=True, load=False, filename=None):\n",
        "    history = {\n",
        "        'loss' : []\n",
        "    }\n",
        "    for metric in metrics.keys():\n",
        "        history[metric] = []\n",
        "\n",
        "    if load:\n",
        "        path = checkpoints_dir + filename\n",
        "        model.dense.load_state_dict(torch.load(path))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for iteration, (tr_X, tr_Y) in enumerate(tr_dataloader):\n",
        "            # Set training mode\n",
        "            model.train()\n",
        "            # Unpack targets and cast to float\n",
        "            tr_start, tr_end = tr_Y\n",
        "            tr_start = tr_start.to(device=model.device, dtype=torch.long)\n",
        "            tr_end = tr_end.to(device=model.device, dtype=torch.long)\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            _, tr_input, tr_attention_mask = tr_X\n",
        "            pred_start, pred_end = model.forward((tr_input, tr_attention_mask))\n",
        "            # Loss function\n",
        "            ## TOCHECK\n",
        "            loss = loss_fn(pred_start, tr_start) + loss_fn(pred_end, tr_end)\n",
        "            # Gradient update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss\n",
        "            history['loss'].append(loss)\n",
        "            if iteration%100 == 0:\n",
        "                print(f'Epoch: {epoch+1} ({iteration+1}/{n_iter}) | Loss: {loss}\\n', end='')\n",
        "            if save:\n",
        "                writer.add_scalar('Loss/train-iterations', loss, epoch*len(tr_dataloader)+iteration)\n",
        "            \n",
        "        # Save logs\n",
        "        if save:\n",
        "            with open(logs_dir +'log.txt', 'a+') as f:\n",
        "                f.write(f\"Epoch: {epoch} [Loss: {loss}]\\n\")\n",
        "\n",
        "        # Evaluation\n",
        "        if (epoch+1) % 3 == 0:\n",
        "            print('Checkpoint reached. Starting evaluation...')\n",
        "            # Set evaluation mode\n",
        "            model.eval()\n",
        "            # Unpack inputs and targets\n",
        "            val_X, val_Y = next(val_dataloader)\n",
        "            _, val_input, val_attention_mask = val_X\n",
        "            # Compute scores\n",
        "            scores = evaluate(model, (val_input, val_attention_mask), val_Y, metrics)\n",
        "            # Track scores\n",
        "            for metric, score in zip(metrics.keys(), scores):\n",
        "                history[metric].append(score)\n",
        "                writer.add_scalar(f'{metric}/train-epochs', score, epoch)\n",
        "            if save:\n",
        "                # Save weights\n",
        "                print('Saving weights')\n",
        "                weights_save_path = checkpoints_dir + 'weights' + utc_string + '_' + str(epoch)\n",
        "                torch.save(model.dense.state_dict(), weights_save_path)\n",
        "            \n",
        "        # Learning Rate scheduler\n",
        "        #scheduler.step()\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w_QfnIV8PuIc",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2022-01-25T16:21:13.055Z"
        },
        "id": "w_QfnIV8PuIc"
      },
      "outputs": [],
      "source": [
        "history = train(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "jRP3HY6sYU1j"
      },
      "id": "jRP3HY6sYU1j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GX17WJLkHUro",
      "metadata": {
        "id": "GX17WJLkHUro"
      },
      "outputs": [],
      "source": [
        "from util.evaluation import evaluate_model\n",
        "\n",
        "evaluate_model(model=net, dataloader=val_dataloader, weights_path=resdir+'/weights/weights20220119_39')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo uncertainty"
      ],
      "metadata": {
        "id": "fIffJhUyYX4Q"
      },
      "id": "fIffJhUyYX4Q"
    },
    {
      "cell_type": "code",
      "source": [
        "def montecarlo_uncertainty(input, model, forward_passes, load=False):\n",
        "  if load:\n",
        "    path = homedir + 'best_model/best_model'\n",
        "    model.dense.load_state_dict(torch.load(path))\n",
        "  \"\"\" Function to get the monte-carlo samples and uncertainty estimates\n",
        "    through multiple forward passes\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_loader : object\n",
        "        data loader object from the data loader module\n",
        "    forward_passes : int\n",
        "        number of monte-carlo samples/forward passes\n",
        "    model : object\n",
        "        keras model\n",
        "    n_classes : int\n",
        "        number of classes in the dataset\n",
        "    n_samples : int\n",
        "        number of samples in the test set\n",
        "    \"\"\"\n",
        "  dropout_predictions_start = np.empty((input[0].size()[0], 512))\n",
        "  dropout_predictions_end = np.empty((input[0].size()[0], 512))\n",
        "  softmax = torch.nn.Softmax(dim=1)\n",
        "  start_inferences = []\n",
        "  end_inferences = []\n",
        "  for i in range(forward_passes):\n",
        "    model.eval()\n",
        "    model.dropout.train()\n",
        "    with torch.no_grad():\n",
        "      output_start, output_end = model(input)\n",
        "      predicted_start = softmax(output_start)\n",
        "      predicted_end = softmax(output_end) # shape (n_samples, n_classes)\n",
        "      start_inferences.append(torch.argmax(predicted_start, axis=1).item())\n",
        "      end_inferences.append(torch.argmax(predicted_end, axis=1).item())\n",
        "\n",
        "    dropout_predictions_start = np.vstack((dropout_predictions_start, predicted_start.cpu().numpy()))\n",
        "    dropout_predictions_end = np.vstack((dropout_predictions_end, predicted_end.cpu().numpy()))\n",
        "  mc_logits_start = dropout_predictions_start[1:]\n",
        "  mc_logits_end = dropout_predictions_end[1:]\n",
        "\n",
        "\n",
        "  # Calculating mean across multiple MCD forward passes \n",
        "  mean_start = np.mean(mc_logits_start, axis=0) # shape (n_samples, n_classes)\n",
        "  mean_end = np.mean(mc_logits_end, axis=0)\n",
        "  # Calculating variance across multiple MCD forward passes \n",
        "  variance = np.var(mc_logits_start, axis=0) # shape (n_samples, n_classes)\n",
        "\n",
        "  epsilon = sys.float_info.min\n",
        "  # Calculating entropy across multiple MCD forward passes \n",
        "  entropy_start = -np.sum(mean_start*np.log(mean_start + epsilon), axis=-1) # shape (n_samples,)\n",
        "  entropy_end = -np.sum(mean_end*np.log(mean_end + epsilon), axis=-1)\n",
        "  return output_start, output_end, entropy_start, entropy_end\n",
        "\n",
        "\n",
        "# Compute Monte Carlo uncertainty for a given dimension batch of input contexts\n",
        "tokenizer=DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "def compute_mc_uncertainty(mc_batch_dim, val_dataset, tokenizer, load=False):\n",
        "  # Building new dataloader with batch_size=1 for inference\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, pin_memory=True)\n",
        "  val_dataloader = iter(val_dataloader)\n",
        "  \n",
        "  # Testing monte carlo\n",
        "  for i in range(mc_batch_dim):\n",
        "    input, target = next(val_dataloader)\n",
        "    _, inp, att = input\n",
        "    mean_start, mean_end, entropy_start, entropy_end = montecarlo_uncertainty((inp, att), net, 5, load=load)\n",
        "    start_pred = int(torch.argmax(mean_start.cpu(), axis=1).detach().numpy().astype('int32'))\n",
        "    end_pred = int(torch.argmax(mean_end.cpu(), axis=1).detach().numpy().astype('int32'))\n",
        "    \n",
        "    # Extract answer from context\n",
        "    context = inp.detach().numpy().reshape(-1,)\n",
        "    pred_decoded = tokenizer.decode(context[start_pred:end_pred+1])\n",
        "    target_decoded = tokenizer.decode(context[target[0]:target[1]+1])\n",
        "    uncertainty_score = (entropy_start+entropy_end)/2\n",
        "    print(f'Predicted answer: {pred_decoded}\\nActual answer: {target_decoded}\\nUncertainty score on the prediction: {round((uncertainty_score*100)/(512/np.exp(1)), 3)}%\\n\\n')"
      ],
      "metadata": {
        "id": "7VfwHQIkGswk"
      },
      "id": "7VfwHQIkGswk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_mc_uncertainty(30, val_dataset, tokenizer, load=True)"
      ],
      "metadata": {
        "id": "C0AEaz2dGlTb"
      },
      "id": "C0AEaz2dGlTb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jaccard Index\n",
        "Custom evaluation metric that computes the percentage of the actual answer predicted by the model\n"
      ],
      "metadata": {
        "id": "xGnXPHNCZa-I"
      },
      "id": "xGnXPHNCZa-I"
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_accuracy_metric(actual_start, actual_end, predicted_start_array, predicted_end_array):\n",
        "  pred_start = torch.argmax(predicted_start_array, axis=1).to(dtype=torch.int32)\n",
        "  pred_end = torch.argmax(predicted_end_array, axis=1).to(dtype=torch.int32)\n",
        "  tot_percentage = 0\n",
        "  jaccard_metric = 0\n",
        "  for i in range(actual_start.size()[0]):\n",
        "    actual = list(range(actual_start[i], actual_end[i]+1))\n",
        "    pred = list(range(pred_start[i], pred_end[i]+1))\n",
        "    # tot_percentage += len(set(pred).intersection(actual))/len(actual)\n",
        "    jaccard_metric += len(set(pred).intersection(actual))/len(set(pred).union(actual))\n",
        "    tot_percentage += len(set(pred).intersection(actual))/len(actual)\n",
        "  \n",
        "  return (tot_percentage/actual_start.size()[0])*100, jaccard_metric/actual_start.size()[0]"
      ],
      "metadata": {
        "id": "aruszQyRDj3_"
      },
      "id": "aruszQyRDj3_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F1 and Exact Match"
      ],
      "metadata": {
        "id": "Xm6zaD0mYhk-"
      },
      "id": "Xm6zaD0mYhk-"
    },
    {
      "cell_type": "code",
      "source": [
        "def post_train_eval(model, inp, att, target):\n",
        "    # Obtain predictions\n",
        "    start_preds, end_preds = model.forward((inp,att))\n",
        "    # Unpack targets and send to device\n",
        "    start_target, end_target = target\n",
        "    start_target = start_target.to(model.device)\n",
        "    end_target = end_target.to(model.device)\n",
        "    \n",
        "#    # Extract IntTensors for predictions\n",
        "#    start_preds, end_preds = torch.zeros_like(start_model, dtype=torch.int16), torch.zeros_like(end_model, dtype=torch.int16)\n",
        "#    start_preds[torch.tensor(range(start_model.size()[0])), torch.argmax(start_model, axis=1)] = 1\n",
        "#    end_preds[torch.tensor(range(end_model.size()[0])), torch.argmax(end_model, axis=1)] = 1\n",
        "\n",
        "    # Send predictions to device\n",
        "    start_preds = start_preds.to(model.device)\n",
        "    end_preds = end_preds.to(model.device)\n",
        "    percentage_accuracy, jaccard = jaccard_accuracy_metric(start_target, end_target, start_preds, end_preds)\n",
        "    return percentage_accuracy, jaccard, start_preds, end_preds, start_target, end_target\n",
        "\n",
        "def final_evaluation(model, batch_size, full_val_dataset=False):\n",
        "    # Building new dataloader with batch_size=1 for inference\n",
        "    path = homedir + 'best_model/best_model'\n",
        "    model.dense.load_state_dict(torch.load(path))\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, pin_memory=True)\n",
        "    val_dataloader = iter(val_dataloader)\n",
        "    # Set evaluation mode\n",
        "    model.eval()\n",
        "    correct_answers_counter = 0\n",
        "    percentage_acc_list = []\n",
        "    jaccard_list = []\n",
        "    start_preds_list = np.empty((1, 512))\n",
        "    end_preds_list = np.empty((1, 512))\n",
        "    start_target_list = []#np.empty((1, 512))\n",
        "    end_target_list = []#np.empty((1, 512))\n",
        "    \n",
        "    if full_val_dataset:\n",
        "      batch_size = len(val_dataloader)\n",
        "    \n",
        "    for i in range(batch_size):#range(len(val_dataloader)):\n",
        "      input, target = next(val_dataloader)\n",
        "      _, inp, att = input\n",
        "      perc, jaccard, st_pred, end_pred, st_tar, end_tar = post_train_eval(model, inp, att, target)\n",
        "      \n",
        "      #print(f'Percentage of answer correct: {perc}')\n",
        "      if perc >= 70:\n",
        "        correct_answers_counter += 1\n",
        "      \n",
        "      correct_answers_counter\n",
        "      percentage_acc_list.append(perc)\n",
        "      jaccard_list.append(jaccard)\n",
        "      start_preds_list = np.vstack((start_preds_list, st_pred.cpu().detach().numpy()))\n",
        "      end_preds_list = np.vstack((end_preds_list, end_pred.cpu().detach().numpy()))\n",
        "      start_target_list.append(st_tar.item())\n",
        "      end_target_list.append(end_tar.item())\n",
        "    \n",
        "    f1_score = metrics['F1']\n",
        "    exact_match = metrics['ExactMatch']\n",
        "\n",
        "    # Get F1 scores\n",
        "    f1_start = f1_score(torch.tensor(start_preds_list[1:]), torch.tensor(np.array(start_target_list)))\n",
        "    f1_end = f1_score(torch.tensor(end_preds_list[1:]), torch.tensor(np.array(end_target_list)))\n",
        "    f1 = (f1_start + f1_end)/2\n",
        "    f1 = f1.to('cpu')\n",
        "  \n",
        "    # Ge Exact Match scores\n",
        "    em_start = exact_match(torch.tensor(start_preds_list[1:]), torch.tensor(np.array(start_target_list)))\n",
        "    em_end = exact_match(torch.tensor(end_preds_list[1:]), torch.tensor(np.array(end_target_list)))\n",
        "    em = (em_start + em_end)/2\n",
        "\n",
        "    print('Evaluation completed.')\n",
        "    print(f'Correct answers: {correct_answers_counter} out of {batch_size} total')\n",
        "    print(f'Avreage Jaccard index on correct answers: {np.mean(np.array(jaccard_list))}')\n",
        "    print(f'F1: {f1}, Exact Match: {em}')\n",
        "                \n",
        "    return f1, em\n",
        "\n",
        "final_evaluation(net, 2000, full_val_dataset=True)"
      ],
      "metadata": {
        "id": "4FKVyQDx6PL1"
      },
      "id": "4FKVyQDx6PL1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show tensorboard of a specfied training\n",
        "tens_dir = '/content/gdrive/My Drive/QA project/runs/'\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"$tens_dir\""
      ],
      "metadata": {
        "id": "VjLIXtQo0Ep-"
      },
      "id": "VjLIXtQo0Ep-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:nlp] *",
      "language": "python",
      "name": "conda-env-nlp-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}