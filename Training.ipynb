{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c69e289",
      "metadata": {
        "id": "3c69e289"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "291d7474",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.154416Z",
          "start_time": "2022-01-12T12:45:57.538962Z"
        },
        "id": "291d7474"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppress output\n",
        "\n",
        "# Whether the notebook is run within Google Colab or not\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "# Install needed dependencies on Colab\n",
        "if colab:\n",
        "    !pip install transformers\n",
        "    !pip install torchmetrics\n",
        "from transformers import DistilBertModel#, DistilBertTokenizerFast\n",
        "\n",
        "# Enable GPU acceleration, whenever available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Automatically reimport modules at each execution\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86992ec3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.223020Z",
          "start_time": "2022-01-12T12:46:05.157398Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86992ec3",
        "outputId": "22835bfc-030d-4302-e565-e5d0e0ea4568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'question-answering'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 132 (delta 60), reused 61 (delta 20), pack-reused 13\u001b[K\n",
            "Receiving objects: 100% (132/132), 32.80 MiB | 4.56 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ],
      "source": [
        "if colab:\n",
        "    !git clone 'https://github.com/michimichiamo/question-answering'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa58304",
      "metadata": {
        "id": "eaa58304"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3124ea1c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.281868Z",
          "start_time": "2022-01-12T12:46:05.226393Z"
        },
        "id": "3124ea1c"
      },
      "outputs": [],
      "source": [
        "# Execute this only to load the dataset in csv format if not already done\n",
        "# from read_dataset import read_dataset\n",
        "\n",
        "# dataset = read_dataset(path='training_set.json', validation_set_perc=20)\n",
        "# train_df = pd.DataFrame(dataset[0], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# train_df.to_csv('train_df.csv')\n",
        "# val_df = pd.DataFrame(dataset[1], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# val_df.to_csv('val_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fb334c6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.344966Z",
          "start_time": "2022-01-12T12:46:05.284455Z"
        },
        "id": "6fb334c6"
      },
      "outputs": [],
      "source": [
        "directory='./' if not colab else './question-answering/'\n",
        "\n",
        "train_filename = directory+'data/tokenized/train.npz'\n",
        "val_filename = directory+'data/tokenized/val.npz'\n",
        "\n",
        "train_data = np.load(train_filename)\n",
        "val_data = np.load(val_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "6b01bd4b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:07.116897Z",
          "start_time": "2022-01-12T12:46:05.347475Z"
        },
        "id": "6b01bd4b"
      },
      "outputs": [],
      "source": [
        "train_input_ids = train_data['input_ids']\n",
        "train_attention_mask = train_data['attention_mask']\n",
        "train_answer_start = train_data['answer_start']\n",
        "train_answer_end = train_data['answer_end']\n",
        "\n",
        "val_input_ids = val_data['input_ids']\n",
        "val_attention_mask = val_data['attention_mask']\n",
        "val_answer_start = val_data['answer_start']\n",
        "val_answer_end = val_data['answer_end']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6205600e",
      "metadata": {
        "id": "6205600e"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4916e6fc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:07.652998Z",
          "start_time": "2022-01-12T12:46:07.593116Z"
        },
        "id": "4916e6fc"
      },
      "outputs": [],
      "source": [
        "class QA(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=768, num_labels=2, dropout_rate=0.5):\n",
        "        super(QA, self).__init__()\n",
        "        # Device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        # Parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        # Layers\n",
        "        #self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "        self.transformers = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad').to(self.device)\n",
        "        self.transformers.requires_grad_(False)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        #self.extra_linear = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        #self.extra_linear_tanh = torch.nn.Tanh()\n",
        "        self.dense = torch.nn.Linear(self.hidden_size, self.num_labels, device=self.device, dtype=torch.int16)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Unpack inputs\n",
        "        input_ids, attention_mask = inputs\n",
        "        \n",
        "        # Put to device\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        \n",
        "        # Transformers \n",
        "        transformed = self.transformers(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Dropout\n",
        "        dropped = self.dropout(transformed[0])\n",
        "        # Obtain logits\n",
        "        logits = self.dense(dropped) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
        "        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
        "        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
        "        # --- 4) Prepare output tuple\n",
        "        outputs = (start_logits, end_logits)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cFqtEIvq9bio",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.796206Z",
          "start_time": "2022-01-12T12:46:07.655319Z"
        },
        "id": "cFqtEIvq9bio"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "net = QA()\n",
        "net.to(net.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ZX7lp9lGApN4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.864157Z",
          "start_time": "2022-01-12T12:46:10.798299Z"
        },
        "id": "ZX7lp9lGApN4"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, input_ids, attention_masks, answer_starts, answer_ends):\n",
        "        'Initialization'\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.answer_starts = answer_starts\n",
        "        self.answer_ends = answer_ends\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        input_id = self.input_ids[index]\n",
        "        attention_mask = self.attention_masks[index]\n",
        "        answer_start = self.answer_starts[index]\n",
        "        answer_end = self.answer_ends[index]\n",
        "\n",
        "        # Pack input and output\n",
        "        X = (input_id, attention_mask)\n",
        "        y = (answer_start, answer_end)\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "batch_size = 32 #@param [\"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "learning_rate = 0.001 #@param [\"0.00001\", \"0.0001\", \"0.001\", \"0.01\", \"0.1\", \"1\"] {type:\"raw\"}\n",
        "epochs = 5 #@param {type:\"slider\", min:5, max:200, step:5}\n"
      ],
      "metadata": {
        "id": "SFKL9c4TZSa3"
      },
      "id": "SFKL9c4TZSa3",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "QqhlR1A9BKsa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.924544Z",
          "start_time": "2022-01-12T12:46:10.866537Z"
        },
        "id": "QqhlR1A9BKsa"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(train_input_ids, train_attention_mask, train_answer_start, train_answer_end)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)#, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = Dataset(val_input_ids, val_attention_mask, val_answer_start, val_answer_end)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, )#, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "arGKmyRmXSlB"
      },
      "id": "arGKmyRmXSlB",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del train_input_ids, train_attention_mask, train_answer_start, train_answer_end\n",
        "del val_input_ids, val_attention_mask, val_answer_start, val_answer_end\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ME4IcRMhb2",
        "outputId": "4a2988f8-c504-445b-d8c3-c46138e71e4b"
      },
      "id": "R-ME4IcRMhb2",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "loss_fn = CrossEntropyLoss()\n",
        "optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "n_iter = round(len(train_dataloader)/(batch_size))"
      ],
      "metadata": {
        "id": "IZpQ_Tn6PnTC"
      },
      "id": "IZpQ_Tn6PnTC",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import AveragePrecision, F1\n",
        "#from torch.nn.functional import softmax\n",
        "\n",
        "def evaluate(model, inputs, targets):\n",
        "    # Set evaluation mode\n",
        "    model.eval()\n",
        "    # Obtain predictions\n",
        "    start_preds, end_preds = model.forward(inputs)\n",
        "    # Unpack targets\n",
        "    start_logits, end_logits = targets\n",
        "    # Get F1 scores\n",
        "    f1_score = F1(num_classes=model.transformers.config.max_position_embeddings)\n",
        "    \n",
        "    # Extract IntTensors\n",
        "    start_out, end_out = torch.zeros_like(start_preds, dtype=torch.int16), torch.zeros_like(end_preds, dtype=torch.int16)\n",
        "    start_out[torch.tensor(range(start_preds.size()[0])), torch.argmax(start_preds, axis=1)] = 1\n",
        "    end_out[torch.tensor(range(end_preds.size()[0])), torch.argmax(end_preds, axis=1)] = 1\n",
        "\n",
        "    f1_start = f1_score(start_logits, start_out)\n",
        "    f1_end = f1_score(end_logits, end_out)\n",
        "    f1 = f1_start + f1_end\n",
        "    \n",
        "    # Get Average Precision scores\n",
        "    average_precision = AveragePrecision(pos_label=1, num_classes=model.transformers.config.max_position_embeddings)\n",
        "\n",
        "    avg_start = average_precision(start_logits, start_preds)\n",
        "    avg_end = average_precision(end_logits, end_preds)\n",
        "    avg = avg_start + avg_end\n",
        "\n",
        "    print(f'f1 score: {f1}')\n",
        "    print(f'average precision: {avg}')\n",
        "    return f1, avg\n"
      ],
      "metadata": {
        "id": "Wiv72WDyWXvC"
      },
      "id": "Wiv72WDyWXvC",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "dhvSBZ549wLG",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T14:39:15.485889Z",
          "start_time": "2022-01-12T12:46:11.068018Z"
        },
        "id": "dhvSBZ549wLG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "97b0f4ea-2692-4ee3-950e-2b011859855d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-975994f30df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_prec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mf1_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mavg_prec_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_prec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8b401da0af50>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, inputs, targets)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mend_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mf1_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mf1_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf1_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_on_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_computed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchmetrics/classification/stat_scores.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         )\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchmetrics/functional/classification/stat_scores.py\u001b[0m in \u001b[0;36m_stat_scores_update\u001b[0;34m(preds, target, reduce, mdmc_reduce, num_classes, top_k, threshold, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     preds, target, _ = _input_format_classification(\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_input_format_classification\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Convert half precision tensors to full precision, as not all ops are supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# for example, min() is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'torch.float16' as a data type"
          ]
        }
      ],
      "source": [
        "loss_history = []\n",
        "f1_history = []\n",
        "avg_prec_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #net.train()\n",
        "    for iteration, (train_inputs, train_targets) in enumerate(train_dataloader):\n",
        "        #net.train()\n",
        "        ## Unpack targets\n",
        "        #start_logits, end_logits = train_targets\n",
        "        ## Forward pass\n",
        "        #optimizer.zero_grad()\n",
        "        #start_out, end_out = net.forward(train_inputs)\n",
        "        ## Loss function\n",
        "        #loss = -loss_fn(start_logits, start_out) -loss_fn(end_logits, end_out)\n",
        "        ## Gradient update\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "#\n",
        "        ## Track loss\n",
        "        #print(f'iteration {iteration+1}/{n_iter}')\n",
        "        #print(f'loss = {loss}')\n",
        "        #loss_history.append(loss)\n",
        "    \n",
        "    #if epoch % 5 == 0:\n",
        "        net.eval()\n",
        "        val_inputs, val_targets = next(iter(val_dataloader))\n",
        "        f1, avg_prec = evaluate(net, val_inputs, val_targets)\n",
        "        f1_history.append(f1)\n",
        "        avg_prec_history.append(avg_prec)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "QA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}