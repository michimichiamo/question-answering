{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c69e289",
      "metadata": {
        "id": "3c69e289"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "291d7474",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.154416Z",
          "start_time": "2022-01-12T12:45:57.538962Z"
        },
        "id": "291d7474"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppress output\n",
        "\n",
        "# Whether the notebook is run within Google Colab or not\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "# Install needed dependencies on Colab\n",
        "if colab:\n",
        "    !pip install transformers\n",
        "    !pip install torchmetrics\n",
        "from transformers import DistilBertModel#, DistilBertTokenizerFast\n",
        "\n",
        "# Enable GPU acceleration, whenever available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Automatically reimport modules at each execution\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86992ec3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.223020Z",
          "start_time": "2022-01-12T12:46:05.157398Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86992ec3",
        "outputId": "d791ce50-e6d3-427c-8a57-aee9f8d5794f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'question-answering' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "if colab:\n",
        "    !git clone 'https://github.com/michimichiamo/question-answering'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa58304",
      "metadata": {
        "id": "eaa58304"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3124ea1c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.281868Z",
          "start_time": "2022-01-12T12:46:05.226393Z"
        },
        "id": "3124ea1c"
      },
      "outputs": [],
      "source": [
        "# Execute this only to load the dataset in csv format if not already done\n",
        "# from read_dataset import read_dataset\n",
        "\n",
        "# dataset = read_dataset(path='training_set.json', validation_set_perc=20)\n",
        "# train_df = pd.DataFrame(dataset[0], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# train_df.to_csv('train_df.csv')\n",
        "# val_df = pd.DataFrame(dataset[1], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
        "# val_df.to_csv('val_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fb334c6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:05.344966Z",
          "start_time": "2022-01-12T12:46:05.284455Z"
        },
        "id": "6fb334c6"
      },
      "outputs": [],
      "source": [
        "directory='./' if not colab else './question-answering/'\n",
        "\n",
        "train_filename = directory+'data/tokenized/train.npz'\n",
        "val_filename = directory+'data/tokenized/val.npz'\n",
        "\n",
        "train_data = np.load(train_filename)\n",
        "val_data = np.load(val_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6b01bd4b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:07.116897Z",
          "start_time": "2022-01-12T12:46:05.347475Z"
        },
        "id": "6b01bd4b"
      },
      "outputs": [],
      "source": [
        "train_input_ids = train_data['input_ids'].astype('int32')\n",
        "train_attention_mask = train_data['attention_mask'].astype('int32')\n",
        "train_answer_start = train_data['answer_start'].astype('int32')\n",
        "train_answer_end = train_data['answer_end'].astype('int32')\n",
        "\n",
        "val_input_ids = val_data['input_ids'].astype('int32')\n",
        "val_attention_mask = val_data['attention_mask'].astype('int32')\n",
        "val_answer_start = val_data['answer_start'].astype('int32')\n",
        "val_answer_end = val_data['answer_end'].astype('int32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6205600e",
      "metadata": {
        "id": "6205600e"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4916e6fc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:07.652998Z",
          "start_time": "2022-01-12T12:46:07.593116Z"
        },
        "id": "4916e6fc"
      },
      "outputs": [],
      "source": [
        "class QA(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=768, num_labels=2, dropout_rate=0.5):\n",
        "        super(QA, self).__init__()\n",
        "        # Device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "        # Parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        # Layers\n",
        "        #self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "        self.transformers = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad').to(self.device)\n",
        "        self.transformers.requires_grad_(False)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        #self.extra_linear = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        #self.extra_linear_tanh = torch.nn.Tanh()\n",
        "        self.dense = torch.nn.Linear(self.hidden_size, self.num_labels, device=self.device, dtype=torch.float32)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Unpack inputs\n",
        "        input_ids, attention_mask = inputs\n",
        "        \n",
        "        # Put to device\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        \n",
        "        # Transformers \n",
        "        transformed = self.transformers(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Dropout\n",
        "        dropped = self.dropout(transformed[0])\n",
        "        # Obtain logits\n",
        "        logits = self.dense(dropped) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
        "        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
        "        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
        "        # --- 4) Prepare output tuple\n",
        "        outputs = (start_logits, end_logits)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ZX7lp9lGApN4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.864157Z",
          "start_time": "2022-01-12T12:46:10.798299Z"
        },
        "id": "ZX7lp9lGApN4"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, input_ids, attention_masks, answer_starts, answer_ends):\n",
        "        'Initialization'\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.answer_starts = answer_starts\n",
        "        self.answer_ends = answer_ends\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        input_id = self.input_ids[index]\n",
        "        attention_mask = self.attention_masks[index]\n",
        "        answer_start = self.answer_starts[index]\n",
        "        answer_end = self.answer_ends[index]\n",
        "\n",
        "        # Pack input and output\n",
        "        X = (input_id, attention_mask)\n",
        "        y = (answer_start, answer_end)\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "batch_size = 256 #@param [\"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "learning_rate = 0.001 #@param [\"0.00001\", \"0.0001\", \"0.001\", \"0.01\", \"0.1\", \"1\"] {type:\"raw\"}\n",
        "epochs = 5 #@param {type:\"slider\", min:5, max:200, step:5}\n"
      ],
      "metadata": {
        "id": "SFKL9c4TZSa3"
      },
      "id": "SFKL9c4TZSa3",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "QqhlR1A9BKsa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T12:46:10.924544Z",
          "start_time": "2022-01-12T12:46:10.866537Z"
        },
        "id": "QqhlR1A9BKsa"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(train_input_ids, train_attention_mask, train_answer_start, train_answer_end)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)#, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = Dataset(val_input_ids, val_attention_mask, val_answer_start, val_answer_end)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, )#, num_workers=2, pin_memory=True)\n",
        "val_dataloader = iter(val_dataloader)"
      ],
      "metadata": {
        "id": "arGKmyRmXSlB"
      },
      "id": "arGKmyRmXSlB",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del train_input_ids, train_attention_mask, train_answer_start, train_answer_end\n",
        "del val_input_ids, val_attention_mask, val_answer_start, val_answer_end\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-ME4IcRMhb2",
        "outputId": "568fb98e-e222-4f7b-bd72-b3982bac428e"
      },
      "id": "R-ME4IcRMhb2",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "437"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Create model\n",
        "net = QA()\n",
        "net.to(net.device)\n",
        "optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "n_iter = len(train_dataloader)"
      ],
      "metadata": {
        "id": "IZpQ_Tn6PnTC"
      },
      "id": "IZpQ_Tn6PnTC",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import AveragePrecision, F1\n",
        "\n",
        "# Define scores and send to device\n",
        "f1_score = F1(num_classes=net.transformers.config.max_position_embeddings, mdmc_average='global')\n",
        "f1_score = f1_score.to(device)\n",
        "average_precision = AveragePrecision(pos_label=1, num_classes=net.transformers.config.max_position_embeddings)\n",
        "average_precision = average_precision.to(device)\n",
        "\n",
        "def evaluate(model, inputs, targets):\n",
        "    # Set evaluation mode\n",
        "    model.eval()\n",
        "    # Obtain predictions\n",
        "    start_preds, end_preds = model.forward(inputs)\n",
        "    # Unpack targets and send to device\n",
        "    start_logits, end_logits = targets\n",
        "    start_logits = start_logits.to(model.device)\n",
        "    end_logits = end_logits.to(model.device)\n",
        "    \n",
        "    # Extract IntTensors for predictions\n",
        "    start_out, end_out = torch.zeros_like(start_preds, dtype=torch.int16), torch.zeros_like(end_preds, dtype=torch.int16)\n",
        "    start_out[torch.tensor(range(start_preds.size()[0])), torch.argmax(start_preds, axis=1)] = 1\n",
        "    end_out[torch.tensor(range(end_preds.size()[0])), torch.argmax(end_preds, axis=1)] = 1\n",
        "\n",
        "    # Send predictions to device\n",
        "    start_out.to(model.device)\n",
        "    end_out.to(model.device)\n",
        "\n",
        "    # Get F1 scores\n",
        "    f1_start = f1_score(start_out, start_logits)\n",
        "    f1_end = f1_score(end_out, end_logits)\n",
        "    f1 = f1_start + f1_end\n",
        "    \n",
        "    # Get Average Precision scores\n",
        "    avg_start = average_precision(start_out, torch.argmax(start_logits, axis=1))\n",
        "    avg_end = average_precision(end_out, torch.argmax(end_logits, axis=1))\n",
        "    avg = avg_start + avg_end\n",
        "\n",
        "    print(f'f1 score: {f1:.10f}')\n",
        "    print(f'average precision: {avg:.5f}')\n",
        "    return f1.to('cpu'), avg.to('cpu')\n"
      ],
      "metadata": {
        "id": "Wiv72WDyWXvC",
        "outputId": "4a8da8cc-996e-4759-a360-dd9d61dbf334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Wiv72WDyWXvC",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dhvSBZ549wLG",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-12T14:39:15.485889Z",
          "start_time": "2022-01-12T12:46:11.068018Z"
        },
        "id": "dhvSBZ549wLG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94c12fbf-d32c-420b-a716-a3b4344cb053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1/279\n",
            "loss = 4.62442684173584\n",
            "iteration 2/279\n",
            "loss = 5.791830062866211\n",
            "iteration 3/279\n",
            "loss = 4.785766124725342\n",
            "iteration 4/279\n",
            "loss = 5.347018718719482\n",
            "iteration 5/279\n",
            "loss = 5.220499038696289\n",
            "iteration 6/279\n",
            "loss = 5.5930986404418945\n",
            "iteration 7/279\n",
            "loss = 4.0742878913879395\n",
            "iteration 8/279\n",
            "loss = 5.150293350219727\n",
            "iteration 9/279\n",
            "loss = 5.935805320739746\n",
            "iteration 10/279\n",
            "loss = 6.723872184753418\n",
            "iteration 11/279\n",
            "loss = 4.715449333190918\n",
            "iteration 12/279\n",
            "loss = 5.010364532470703\n",
            "iteration 13/279\n",
            "loss = 6.722583770751953\n",
            "iteration 14/279\n",
            "loss = 6.430431842803955\n",
            "iteration 15/279\n",
            "loss = 6.160817623138428\n",
            "iteration 16/279\n",
            "loss = 3.97371244430542\n",
            "iteration 17/279\n",
            "loss = 4.640641689300537\n",
            "iteration 18/279\n",
            "loss = 4.438995361328125\n",
            "iteration 19/279\n",
            "loss = 4.902169704437256\n",
            "iteration 20/279\n",
            "loss = 7.108757972717285\n",
            "iteration 21/279\n",
            "loss = 7.52585506439209\n",
            "iteration 22/279\n",
            "loss = 5.481133460998535\n",
            "iteration 23/279\n",
            "loss = 5.4608659744262695\n",
            "iteration 24/279\n",
            "loss = 7.400188446044922\n",
            "iteration 25/279\n",
            "loss = 7.688434600830078\n",
            "iteration 26/279\n",
            "loss = 5.15746545791626\n",
            "iteration 27/279\n",
            "loss = 6.698805332183838\n",
            "iteration 28/279\n",
            "loss = 6.467576503753662\n",
            "iteration 29/279\n",
            "loss = 6.602912425994873\n",
            "iteration 30/279\n",
            "loss = 5.682193756103516\n",
            "iteration 31/279\n",
            "loss = 5.21112585067749\n",
            "iteration 32/279\n",
            "loss = 5.158492565155029\n",
            "iteration 33/279\n",
            "loss = 3.9591007232666016\n",
            "iteration 34/279\n",
            "loss = 5.2368340492248535\n",
            "iteration 35/279\n",
            "loss = 8.241279602050781\n",
            "iteration 36/279\n",
            "loss = 7.332095146179199\n",
            "iteration 37/279\n",
            "loss = 5.423884391784668\n",
            "iteration 38/279\n",
            "loss = 4.390323638916016\n",
            "iteration 39/279\n",
            "loss = 4.341489791870117\n",
            "iteration 40/279\n",
            "loss = 4.687824249267578\n",
            "iteration 41/279\n",
            "loss = 5.288858890533447\n",
            "iteration 42/279\n",
            "loss = 4.147663116455078\n",
            "iteration 43/279\n",
            "loss = 6.608121871948242\n",
            "iteration 44/279\n",
            "loss = 4.668752193450928\n",
            "iteration 45/279\n",
            "loss = 5.253770351409912\n",
            "iteration 46/279\n",
            "loss = 4.3243489265441895\n",
            "iteration 47/279\n",
            "loss = 5.666060447692871\n",
            "iteration 48/279\n",
            "loss = 5.530695915222168\n",
            "iteration 49/279\n",
            "loss = 6.939532279968262\n",
            "iteration 50/279\n",
            "loss = 6.220949649810791\n",
            "iteration 51/279\n",
            "loss = 5.212531089782715\n",
            "iteration 52/279\n",
            "loss = 5.800815105438232\n",
            "iteration 53/279\n",
            "loss = 6.416833877563477\n",
            "iteration 54/279\n",
            "loss = 4.172601222991943\n",
            "iteration 55/279\n",
            "loss = 5.74198579788208\n",
            "iteration 56/279\n",
            "loss = 6.025624752044678\n",
            "iteration 57/279\n",
            "loss = 8.04156494140625\n",
            "iteration 58/279\n",
            "loss = 5.217657089233398\n",
            "iteration 59/279\n",
            "loss = 4.449289798736572\n",
            "iteration 60/279\n",
            "loss = 6.052682399749756\n",
            "iteration 61/279\n",
            "loss = 3.6302618980407715\n",
            "iteration 62/279\n",
            "loss = 3.8220014572143555\n",
            "iteration 63/279\n",
            "loss = 6.685976982116699\n",
            "iteration 64/279\n",
            "loss = 4.679592609405518\n",
            "iteration 65/279\n",
            "loss = 7.902152061462402\n",
            "iteration 66/279\n",
            "loss = 5.2706828117370605\n",
            "iteration 67/279\n",
            "loss = 6.542766571044922\n",
            "iteration 68/279\n",
            "loss = 8.490545272827148\n",
            "iteration 69/279\n",
            "loss = 5.358095169067383\n",
            "iteration 70/279\n",
            "loss = 5.265351295471191\n",
            "iteration 71/279\n",
            "loss = 7.726540565490723\n",
            "iteration 72/279\n",
            "loss = 6.097127437591553\n",
            "iteration 73/279\n",
            "loss = 3.9095489978790283\n",
            "iteration 74/279\n",
            "loss = 5.162439346313477\n",
            "iteration 75/279\n",
            "loss = 4.060965538024902\n",
            "iteration 76/279\n",
            "loss = 3.712268590927124\n",
            "iteration 77/279\n",
            "loss = 4.515406131744385\n",
            "iteration 78/279\n",
            "loss = 8.458356857299805\n",
            "iteration 79/279\n",
            "loss = 7.64437198638916\n",
            "iteration 80/279\n",
            "loss = 4.868251323699951\n",
            "iteration 81/279\n",
            "loss = 6.827278137207031\n",
            "iteration 82/279\n",
            "loss = 4.915100574493408\n",
            "iteration 83/279\n",
            "loss = 5.545477390289307\n",
            "iteration 84/279\n",
            "loss = 5.97737455368042\n",
            "iteration 85/279\n",
            "loss = 4.680915832519531\n",
            "iteration 86/279\n",
            "loss = 4.210585117340088\n",
            "iteration 87/279\n",
            "loss = 5.40986967086792\n",
            "iteration 88/279\n",
            "loss = 5.949768543243408\n",
            "iteration 89/279\n",
            "loss = 6.529410362243652\n",
            "iteration 90/279\n",
            "loss = 5.0926313400268555\n",
            "iteration 91/279\n",
            "loss = 5.184791564941406\n",
            "iteration 92/279\n",
            "loss = 5.672104835510254\n",
            "iteration 93/279\n",
            "loss = 5.876959800720215\n",
            "iteration 94/279\n",
            "loss = 3.339696168899536\n",
            "iteration 95/279\n",
            "loss = 3.3705873489379883\n",
            "iteration 96/279\n",
            "loss = 4.732819557189941\n",
            "iteration 97/279\n",
            "loss = 5.4658284187316895\n",
            "iteration 98/279\n",
            "loss = 6.319541931152344\n",
            "iteration 99/279\n",
            "loss = 5.860099792480469\n",
            "iteration 100/279\n",
            "loss = 4.988913536071777\n",
            "iteration 101/279\n",
            "loss = 7.6011152267456055\n",
            "iteration 102/279\n",
            "loss = 6.774186134338379\n",
            "iteration 103/279\n",
            "loss = 4.305737495422363\n",
            "iteration 104/279\n",
            "loss = 5.093656063079834\n",
            "iteration 105/279\n",
            "loss = 4.872829437255859\n",
            "iteration 106/279\n",
            "loss = 5.033937931060791\n",
            "iteration 107/279\n",
            "loss = 5.1952667236328125\n",
            "iteration 108/279\n",
            "loss = 5.558838844299316\n",
            "iteration 109/279\n",
            "loss = 5.217290878295898\n",
            "iteration 110/279\n",
            "loss = 5.148721694946289\n",
            "iteration 111/279\n",
            "loss = 4.207432746887207\n",
            "iteration 112/279\n",
            "loss = 5.597407817840576\n",
            "iteration 113/279\n",
            "loss = 4.423635482788086\n",
            "iteration 114/279\n",
            "loss = 5.313586235046387\n",
            "iteration 115/279\n",
            "loss = 5.620402812957764\n",
            "iteration 116/279\n",
            "loss = 5.495696067810059\n",
            "iteration 117/279\n",
            "loss = 3.0343480110168457\n",
            "iteration 118/279\n",
            "loss = 3.76218318939209\n",
            "iteration 119/279\n",
            "loss = 8.008295059204102\n",
            "iteration 120/279\n",
            "loss = 7.936149597167969\n",
            "iteration 121/279\n",
            "loss = 6.245242595672607\n",
            "iteration 122/279\n",
            "loss = 12.144881248474121\n",
            "iteration 123/279\n",
            "loss = 5.338186264038086\n",
            "iteration 124/279\n",
            "loss = 5.454069137573242\n",
            "iteration 125/279\n",
            "loss = 5.6264495849609375\n",
            "iteration 126/279\n",
            "loss = 6.118168354034424\n",
            "iteration 127/279\n",
            "loss = 5.800263404846191\n",
            "iteration 128/279\n",
            "loss = 4.272572994232178\n",
            "iteration 129/279\n",
            "loss = 4.403700828552246\n",
            "iteration 130/279\n",
            "loss = 3.525580883026123\n",
            "iteration 131/279\n",
            "loss = 4.761859893798828\n",
            "iteration 132/279\n",
            "loss = 8.880463600158691\n",
            "iteration 133/279\n",
            "loss = 5.275928497314453\n",
            "iteration 134/279\n",
            "loss = 4.522437572479248\n",
            "iteration 135/279\n",
            "loss = 10.907106399536133\n",
            "iteration 136/279\n",
            "loss = 4.752580642700195\n",
            "iteration 137/279\n",
            "loss = 5.43474006652832\n",
            "iteration 138/279\n",
            "loss = 5.3018341064453125\n",
            "iteration 139/279\n",
            "loss = 6.474001884460449\n",
            "iteration 140/279\n",
            "loss = 7.033608436584473\n",
            "iteration 141/279\n",
            "loss = 4.872029781341553\n",
            "iteration 142/279\n",
            "loss = 4.523096084594727\n",
            "iteration 143/279\n",
            "loss = 5.183502197265625\n",
            "iteration 144/279\n",
            "loss = 9.584602355957031\n",
            "iteration 145/279\n",
            "loss = 7.103847503662109\n",
            "iteration 146/279\n",
            "loss = 4.987542629241943\n",
            "iteration 147/279\n",
            "loss = 6.726290702819824\n",
            "iteration 148/279\n",
            "loss = 6.031394958496094\n",
            "iteration 149/279\n",
            "loss = 5.581592559814453\n",
            "iteration 150/279\n",
            "loss = 5.534167289733887\n",
            "iteration 151/279\n",
            "loss = 5.26586389541626\n",
            "iteration 152/279\n",
            "loss = 8.122138977050781\n",
            "iteration 153/279\n",
            "loss = 6.268011093139648\n",
            "iteration 154/279\n",
            "loss = 4.334563255310059\n",
            "iteration 155/279\n",
            "loss = 6.76506233215332\n",
            "iteration 156/279\n",
            "loss = 6.107181549072266\n",
            "iteration 157/279\n",
            "loss = 6.4104766845703125\n",
            "iteration 158/279\n",
            "loss = 6.6396403312683105\n",
            "iteration 159/279\n",
            "loss = 6.373390197753906\n",
            "iteration 160/279\n",
            "loss = 5.7387518882751465\n",
            "iteration 161/279\n",
            "loss = 5.386176109313965\n",
            "iteration 162/279\n",
            "loss = 5.948827743530273\n",
            "iteration 163/279\n",
            "loss = 12.166050910949707\n",
            "iteration 164/279\n",
            "loss = 5.961082458496094\n",
            "iteration 165/279\n",
            "loss = 9.079407691955566\n",
            "iteration 166/279\n",
            "loss = 6.812380790710449\n",
            "iteration 167/279\n",
            "loss = 4.794157028198242\n",
            "iteration 168/279\n",
            "loss = 7.464489459991455\n",
            "iteration 169/279\n",
            "loss = 6.402099609375\n",
            "iteration 170/279\n",
            "loss = 5.588979244232178\n",
            "iteration 171/279\n",
            "loss = 6.5427703857421875\n",
            "iteration 172/279\n",
            "loss = 7.560680389404297\n",
            "iteration 173/279\n",
            "loss = 8.963054656982422\n",
            "iteration 174/279\n",
            "loss = 8.967571258544922\n",
            "iteration 175/279\n",
            "loss = 9.729937553405762\n",
            "iteration 176/279\n",
            "loss = 6.507883071899414\n",
            "iteration 177/279\n",
            "loss = 4.918974876403809\n",
            "iteration 178/279\n",
            "loss = 7.3653483390808105\n",
            "iteration 179/279\n",
            "loss = 7.383605480194092\n",
            "iteration 180/279\n",
            "loss = 5.749179840087891\n",
            "iteration 181/279\n",
            "loss = 6.854517459869385\n",
            "iteration 182/279\n",
            "loss = 5.862199783325195\n",
            "iteration 183/279\n",
            "loss = 10.686598777770996\n",
            "iteration 184/279\n",
            "loss = 4.9625654220581055\n",
            "iteration 185/279\n",
            "loss = 8.315122604370117\n",
            "iteration 186/279\n",
            "loss = 7.879408359527588\n",
            "iteration 187/279\n",
            "loss = 8.681510925292969\n",
            "iteration 188/279\n",
            "loss = 5.277024269104004\n",
            "iteration 189/279\n",
            "loss = 5.894660949707031\n",
            "iteration 190/279\n",
            "loss = 6.474291801452637\n",
            "iteration 191/279\n",
            "loss = 8.262323379516602\n",
            "iteration 192/279\n",
            "loss = 5.005234241485596\n",
            "iteration 193/279\n",
            "loss = 8.020606994628906\n",
            "iteration 194/279\n",
            "loss = 5.964940071105957\n",
            "iteration 195/279\n",
            "loss = 3.9436213970184326\n",
            "iteration 196/279\n",
            "loss = 4.306751251220703\n",
            "iteration 197/279\n",
            "loss = 4.285257339477539\n",
            "iteration 198/279\n",
            "loss = 5.311219215393066\n",
            "iteration 199/279\n",
            "loss = 8.748335838317871\n",
            "iteration 200/279\n",
            "loss = 4.798361778259277\n",
            "iteration 201/279\n",
            "loss = 6.760166168212891\n",
            "iteration 202/279\n",
            "loss = 8.152033805847168\n",
            "iteration 203/279\n",
            "loss = 6.259500503540039\n",
            "iteration 204/279\n",
            "loss = 6.216582298278809\n",
            "iteration 205/279\n",
            "loss = 5.678484916687012\n",
            "iteration 206/279\n",
            "loss = 7.003981113433838\n",
            "iteration 207/279\n",
            "loss = 4.754162311553955\n",
            "iteration 208/279\n",
            "loss = 3.733038902282715\n",
            "iteration 209/279\n",
            "loss = 4.394096374511719\n",
            "iteration 210/279\n",
            "loss = 4.21934700012207\n",
            "iteration 211/279\n",
            "loss = 4.521869659423828\n",
            "iteration 212/279\n",
            "loss = 7.21201229095459\n",
            "iteration 213/279\n",
            "loss = 7.0042338371276855\n",
            "iteration 214/279\n",
            "loss = 7.029759407043457\n",
            "iteration 215/279\n",
            "loss = 7.221445083618164\n",
            "iteration 216/279\n",
            "loss = 7.0776190757751465\n",
            "iteration 217/279\n",
            "loss = 5.825870513916016\n",
            "iteration 218/279\n",
            "loss = 6.14658784866333\n",
            "iteration 219/279\n",
            "loss = 5.574881553649902\n",
            "iteration 220/279\n",
            "loss = 6.3331499099731445\n",
            "iteration 221/279\n",
            "loss = 6.531990051269531\n",
            "iteration 222/279\n",
            "loss = 3.911332130432129\n",
            "iteration 223/279\n",
            "loss = 4.465916633605957\n",
            "iteration 224/279\n",
            "loss = 5.06367301940918\n",
            "iteration 225/279\n",
            "loss = 9.700407028198242\n",
            "iteration 226/279\n",
            "loss = 3.8067071437835693\n",
            "iteration 227/279\n",
            "loss = 5.73231315612793\n",
            "iteration 228/279\n",
            "loss = 4.166140556335449\n",
            "iteration 229/279\n",
            "loss = 6.146056652069092\n",
            "iteration 230/279\n",
            "loss = 6.673353672027588\n",
            "iteration 231/279\n",
            "loss = 6.00629997253418\n",
            "iteration 232/279\n",
            "loss = 5.286602973937988\n",
            "iteration 233/279\n",
            "loss = 7.905688285827637\n",
            "iteration 234/279\n",
            "loss = 7.3651041984558105\n",
            "iteration 235/279\n",
            "loss = 7.1545867919921875\n",
            "iteration 236/279\n",
            "loss = 11.74626350402832\n",
            "iteration 237/279\n",
            "loss = 5.325954437255859\n",
            "iteration 238/279\n",
            "loss = 6.0885772705078125\n",
            "iteration 239/279\n",
            "loss = 8.555368423461914\n",
            "iteration 240/279\n",
            "loss = 9.224550247192383\n",
            "iteration 241/279\n",
            "loss = 6.682137966156006\n",
            "iteration 242/279\n",
            "loss = 6.536571025848389\n",
            "iteration 243/279\n",
            "loss = 6.015688419342041\n",
            "iteration 244/279\n",
            "loss = 4.783060550689697\n",
            "iteration 245/279\n",
            "loss = 4.7843828201293945\n",
            "iteration 246/279\n",
            "loss = 4.836328029632568\n",
            "iteration 247/279\n",
            "loss = 7.3465094566345215\n",
            "iteration 248/279\n",
            "loss = 4.541755676269531\n",
            "iteration 249/279\n",
            "loss = 4.353139400482178\n",
            "iteration 250/279\n",
            "loss = 3.9882140159606934\n",
            "iteration 251/279\n",
            "loss = 5.085676193237305\n",
            "iteration 252/279\n",
            "loss = 6.277153968811035\n",
            "iteration 253/279\n",
            "loss = 7.698539733886719\n",
            "iteration 254/279\n",
            "loss = 7.956753730773926\n",
            "iteration 255/279\n",
            "loss = 6.584515571594238\n",
            "iteration 256/279\n",
            "loss = 5.149689674377441\n",
            "iteration 257/279\n",
            "loss = 6.509300231933594\n",
            "iteration 258/279\n",
            "loss = 10.831595420837402\n",
            "iteration 259/279\n",
            "loss = 4.981797218322754\n",
            "iteration 260/279\n",
            "loss = 5.964686393737793\n",
            "iteration 261/279\n",
            "loss = 5.704622268676758\n",
            "iteration 262/279\n",
            "loss = 5.133584022521973\n",
            "iteration 263/279\n",
            "loss = 5.587087631225586\n",
            "iteration 264/279\n",
            "loss = 6.735729217529297\n",
            "iteration 265/279\n",
            "loss = 6.207223892211914\n",
            "iteration 266/279\n",
            "loss = 5.591400146484375\n",
            "iteration 267/279\n",
            "loss = 4.860201835632324\n",
            "iteration 268/279\n",
            "loss = 5.316343307495117\n",
            "iteration 269/279\n",
            "loss = 5.570555686950684\n",
            "iteration 270/279\n",
            "loss = 5.272474765777588\n",
            "iteration 271/279\n",
            "loss = 5.315282821655273\n",
            "iteration 272/279\n",
            "loss = 7.504261493682861\n",
            "iteration 273/279\n",
            "loss = 4.225164890289307\n",
            "iteration 274/279\n",
            "loss = 3.564209461212158\n",
            "iteration 275/279\n",
            "loss = 4.945608139038086\n",
            "iteration 276/279\n",
            "loss = 5.342673301696777\n",
            "iteration 277/279\n",
            "loss = 5.56448221206665\n",
            "iteration 278/279\n",
            "loss = 4.244651794433594\n",
            "iteration 279/279\n",
            "loss = 3.428987503051758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Argument `pos_label` should be `None` when running multiclass precision recall curve. Got 1\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/torchmetrics/functional/classification/average_precision.py:168: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in average\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score: 1.9963531494\n",
            "average precision: 0.81714\n",
            "iteration 1/279\n",
            "loss = 4.759305953979492\n",
            "iteration 2/279\n",
            "loss = 6.479163646697998\n",
            "iteration 3/279\n",
            "loss = 5.152984142303467\n",
            "iteration 4/279\n",
            "loss = 5.579827308654785\n",
            "iteration 5/279\n",
            "loss = 5.713690280914307\n",
            "iteration 6/279\n",
            "loss = 6.076764106750488\n",
            "iteration 7/279\n",
            "loss = 4.0215864181518555\n",
            "iteration 8/279\n",
            "loss = 5.831201553344727\n",
            "iteration 9/279\n",
            "loss = 5.874209403991699\n",
            "iteration 10/279\n",
            "loss = 7.099842548370361\n",
            "iteration 11/279\n",
            "loss = 4.767147064208984\n",
            "iteration 12/279\n",
            "loss = 4.958802223205566\n",
            "iteration 13/279\n",
            "loss = 6.635282516479492\n",
            "iteration 14/279\n",
            "loss = 6.690831184387207\n",
            "iteration 15/279\n",
            "loss = 6.444301605224609\n",
            "iteration 16/279\n",
            "loss = 3.9566280841827393\n",
            "iteration 17/279\n",
            "loss = 4.724586009979248\n",
            "iteration 18/279\n",
            "loss = 4.463770866394043\n",
            "iteration 19/279\n",
            "loss = 5.2642645835876465\n",
            "iteration 20/279\n",
            "loss = 7.166962623596191\n",
            "iteration 21/279\n",
            "loss = 7.462397575378418\n",
            "iteration 22/279\n",
            "loss = 5.762126922607422\n",
            "iteration 23/279\n",
            "loss = 5.792386054992676\n",
            "iteration 24/279\n",
            "loss = 7.448906898498535\n",
            "iteration 25/279\n",
            "loss = 8.1513090133667\n",
            "iteration 26/279\n",
            "loss = 5.185606956481934\n",
            "iteration 27/279\n",
            "loss = 6.596492290496826\n",
            "iteration 28/279\n",
            "loss = 6.645766258239746\n",
            "iteration 29/279\n",
            "loss = 7.0720109939575195\n",
            "iteration 30/279\n",
            "loss = 5.920874118804932\n",
            "iteration 31/279\n",
            "loss = 4.824868202209473\n",
            "iteration 32/279\n",
            "loss = 5.025576114654541\n",
            "iteration 33/279\n",
            "loss = 4.349178314208984\n",
            "iteration 34/279\n",
            "loss = 5.221879959106445\n",
            "iteration 35/279\n",
            "loss = 7.962700366973877\n",
            "iteration 36/279\n",
            "loss = 7.492921352386475\n",
            "iteration 37/279\n",
            "loss = 5.648716926574707\n",
            "iteration 38/279\n",
            "loss = 4.084524631500244\n",
            "iteration 39/279\n",
            "loss = 4.197829723358154\n",
            "iteration 40/279\n",
            "loss = 4.970870018005371\n",
            "iteration 41/279\n",
            "loss = 5.148738861083984\n",
            "iteration 42/279\n",
            "loss = 4.302692413330078\n",
            "iteration 43/279\n",
            "loss = 5.913433074951172\n",
            "iteration 44/279\n",
            "loss = 4.88966178894043\n",
            "iteration 45/279\n",
            "loss = 5.5784807205200195\n",
            "iteration 46/279\n",
            "loss = 4.439627647399902\n",
            "iteration 47/279\n",
            "loss = 5.636745929718018\n",
            "iteration 48/279\n",
            "loss = 4.979427337646484\n",
            "iteration 49/279\n",
            "loss = 6.446371555328369\n",
            "iteration 50/279\n",
            "loss = 5.8755292892456055\n",
            "iteration 51/279\n",
            "loss = 4.817242622375488\n",
            "iteration 52/279\n",
            "loss = 5.981831073760986\n",
            "iteration 53/279\n",
            "loss = 6.4287614822387695\n",
            "iteration 54/279\n",
            "loss = 4.037297248840332\n",
            "iteration 55/279\n",
            "loss = 5.055864334106445\n",
            "iteration 56/279\n",
            "loss = 5.415184020996094\n",
            "iteration 57/279\n",
            "loss = 7.662220478057861\n",
            "iteration 58/279\n",
            "loss = 5.3494791984558105\n",
            "iteration 59/279\n",
            "loss = 4.957973003387451\n",
            "iteration 60/279\n",
            "loss = 6.586711883544922\n",
            "iteration 61/279\n",
            "loss = 3.7709197998046875\n",
            "iteration 62/279\n",
            "loss = 4.031890392303467\n",
            "iteration 63/279\n",
            "loss = 6.668500900268555\n",
            "iteration 64/279\n",
            "loss = 4.46094274520874\n",
            "iteration 65/279\n",
            "loss = 8.336009979248047\n",
            "iteration 66/279\n",
            "loss = 5.3216400146484375\n",
            "iteration 67/279\n",
            "loss = 6.356498718261719\n",
            "iteration 68/279\n",
            "loss = 8.321956634521484\n",
            "iteration 69/279\n",
            "loss = 5.241299152374268\n",
            "iteration 70/279\n",
            "loss = 5.810016632080078\n",
            "iteration 71/279\n",
            "loss = 7.479353904724121\n",
            "iteration 72/279\n",
            "loss = 6.052462577819824\n",
            "iteration 73/279\n",
            "loss = 4.081704139709473\n",
            "iteration 74/279\n",
            "loss = 5.120858192443848\n",
            "iteration 75/279\n",
            "loss = 4.208599090576172\n",
            "iteration 76/279\n",
            "loss = 3.914026975631714\n",
            "iteration 77/279\n",
            "loss = 4.401871204376221\n",
            "iteration 78/279\n",
            "loss = 8.120048522949219\n",
            "iteration 79/279\n",
            "loss = 7.375934600830078\n",
            "iteration 80/279\n",
            "loss = 4.780069351196289\n",
            "iteration 81/279\n",
            "loss = 6.4544758796691895\n",
            "iteration 82/279\n",
            "loss = 5.482325077056885\n",
            "iteration 83/279\n",
            "loss = 5.756824493408203\n",
            "iteration 84/279\n",
            "loss = 6.029622554779053\n",
            "iteration 85/279\n",
            "loss = 4.843685626983643\n",
            "iteration 86/279\n",
            "loss = 3.9954307079315186\n",
            "iteration 87/279\n",
            "loss = 5.37225341796875\n",
            "iteration 88/279\n",
            "loss = 6.047835350036621\n",
            "iteration 89/279\n",
            "loss = 6.560472011566162\n",
            "iteration 90/279\n",
            "loss = 5.1041178703308105\n",
            "iteration 91/279\n",
            "loss = 5.152777671813965\n",
            "iteration 92/279\n",
            "loss = 5.229413032531738\n",
            "iteration 93/279\n",
            "loss = 6.376308441162109\n",
            "iteration 94/279\n",
            "loss = 3.0431089401245117\n",
            "iteration 95/279\n",
            "loss = 3.5916523933410645\n",
            "iteration 96/279\n",
            "loss = 4.655206203460693\n",
            "iteration 97/279\n",
            "loss = 5.634512424468994\n",
            "iteration 98/279\n",
            "loss = 6.1440558433532715\n",
            "iteration 99/279\n",
            "loss = 5.684955596923828\n",
            "iteration 100/279\n",
            "loss = 4.776000022888184\n",
            "iteration 101/279\n",
            "loss = 8.145837783813477\n",
            "iteration 102/279\n",
            "loss = 6.992364406585693\n",
            "iteration 103/279\n",
            "loss = 4.5360589027404785\n",
            "iteration 104/279\n",
            "loss = 4.916662216186523\n",
            "iteration 105/279\n",
            "loss = 4.6284990310668945\n",
            "iteration 106/279\n",
            "loss = 4.680858135223389\n",
            "iteration 107/279\n",
            "loss = 5.576723098754883\n",
            "iteration 108/279\n",
            "loss = 5.352187156677246\n",
            "iteration 109/279\n",
            "loss = 5.280233383178711\n",
            "iteration 110/279\n",
            "loss = 5.220921516418457\n",
            "iteration 111/279\n",
            "loss = 3.8792243003845215\n",
            "iteration 112/279\n",
            "loss = 5.548795223236084\n",
            "iteration 113/279\n",
            "loss = 4.364221572875977\n",
            "iteration 114/279\n",
            "loss = 5.648824691772461\n",
            "iteration 115/279\n",
            "loss = 5.52723503112793\n",
            "iteration 116/279\n",
            "loss = 5.304912090301514\n",
            "iteration 117/279\n",
            "loss = 2.7957394123077393\n",
            "iteration 118/279\n",
            "loss = 3.481616973876953\n",
            "iteration 119/279\n",
            "loss = 7.716176509857178\n",
            "iteration 120/279\n",
            "loss = 8.64448356628418\n",
            "iteration 121/279\n",
            "loss = 5.940758228302002\n",
            "iteration 122/279\n",
            "loss = 11.949029922485352\n",
            "iteration 123/279\n",
            "loss = 5.713006973266602\n",
            "iteration 124/279\n",
            "loss = 5.179214954376221\n",
            "iteration 125/279\n",
            "loss = 5.379192352294922\n",
            "iteration 126/279\n",
            "loss = 5.757265090942383\n",
            "iteration 127/279\n",
            "loss = 5.789913654327393\n",
            "iteration 128/279\n",
            "loss = 4.126650810241699\n",
            "iteration 129/279\n",
            "loss = 4.342761516571045\n",
            "iteration 130/279\n",
            "loss = 3.8210463523864746\n",
            "iteration 131/279\n",
            "loss = 4.493592739105225\n",
            "iteration 132/279\n",
            "loss = 8.642622947692871\n",
            "iteration 133/279\n",
            "loss = 4.945656776428223\n",
            "iteration 134/279\n",
            "loss = 4.285488128662109\n",
            "iteration 135/279\n",
            "loss = 10.68214225769043\n",
            "iteration 136/279\n",
            "loss = 5.021714687347412\n",
            "iteration 137/279\n",
            "loss = 5.630707263946533\n",
            "iteration 138/279\n",
            "loss = 4.755915641784668\n",
            "iteration 139/279\n",
            "loss = 6.047948360443115\n",
            "iteration 140/279\n",
            "loss = 7.355950355529785\n",
            "iteration 141/279\n",
            "loss = 4.740200996398926\n",
            "iteration 142/279\n",
            "loss = 4.605836868286133\n",
            "iteration 143/279\n",
            "loss = 5.593564987182617\n",
            "iteration 144/279\n",
            "loss = 9.03687858581543\n",
            "iteration 145/279\n",
            "loss = 6.874626159667969\n",
            "iteration 146/279\n",
            "loss = 4.882269859313965\n",
            "iteration 147/279\n",
            "loss = 6.419103622436523\n",
            "iteration 148/279\n",
            "loss = 6.13358211517334\n",
            "iteration 149/279\n",
            "loss = 5.736138820648193\n",
            "iteration 150/279\n",
            "loss = 5.685240745544434\n",
            "iteration 151/279\n",
            "loss = 4.9304399490356445\n",
            "iteration 152/279\n",
            "loss = 8.446996688842773\n",
            "iteration 153/279\n",
            "loss = 6.232231616973877\n",
            "iteration 154/279\n",
            "loss = 4.517351150512695\n",
            "iteration 155/279\n",
            "loss = 7.101057052612305\n",
            "iteration 156/279\n",
            "loss = 6.119096755981445\n",
            "iteration 157/279\n",
            "loss = 6.045450687408447\n",
            "iteration 158/279\n",
            "loss = 6.484520435333252\n",
            "iteration 159/279\n",
            "loss = 6.422393321990967\n",
            "iteration 160/279\n",
            "loss = 6.149686813354492\n",
            "iteration 161/279\n",
            "loss = 5.589550018310547\n",
            "iteration 162/279\n",
            "loss = 5.566877841949463\n",
            "iteration 163/279\n",
            "loss = 11.727483749389648\n",
            "iteration 164/279\n",
            "loss = 5.566362380981445\n",
            "iteration 165/279\n",
            "loss = 9.55357551574707\n",
            "iteration 166/279\n",
            "loss = 6.616034507751465\n",
            "iteration 167/279\n",
            "loss = 4.649016380310059\n",
            "iteration 168/279\n",
            "loss = 7.832912921905518\n",
            "iteration 169/279\n",
            "loss = 6.568901062011719\n",
            "iteration 170/279\n",
            "loss = 5.792486190795898\n",
            "iteration 171/279\n",
            "loss = 6.163687229156494\n",
            "iteration 172/279\n",
            "loss = 7.417342185974121\n",
            "iteration 173/279\n",
            "loss = 8.634062767028809\n",
            "iteration 174/279\n",
            "loss = 8.66872501373291\n",
            "iteration 175/279\n",
            "loss = 9.880178451538086\n",
            "iteration 176/279\n",
            "loss = 6.500495910644531\n",
            "iteration 177/279\n",
            "loss = 5.080188751220703\n",
            "iteration 178/279\n",
            "loss = 7.28011417388916\n",
            "iteration 179/279\n",
            "loss = 6.927825927734375\n",
            "iteration 180/279\n",
            "loss = 5.742650032043457\n",
            "iteration 181/279\n",
            "loss = 6.5339741706848145\n",
            "iteration 182/279\n",
            "loss = 6.276767730712891\n",
            "iteration 183/279\n",
            "loss = 10.740507125854492\n",
            "iteration 184/279\n",
            "loss = 4.787773132324219\n",
            "iteration 185/279\n",
            "loss = 8.366937637329102\n",
            "iteration 186/279\n",
            "loss = 7.1084089279174805\n",
            "iteration 187/279\n",
            "loss = 8.572640419006348\n",
            "iteration 188/279\n",
            "loss = 5.179060459136963\n",
            "iteration 189/279\n",
            "loss = 5.690188407897949\n",
            "iteration 190/279\n",
            "loss = 5.932934761047363\n",
            "iteration 191/279\n",
            "loss = 8.175296783447266\n",
            "iteration 192/279\n",
            "loss = 5.177709102630615\n",
            "iteration 193/279\n",
            "loss = 7.85575532913208\n",
            "iteration 194/279\n",
            "loss = 5.788430213928223\n",
            "iteration 195/279\n",
            "loss = 3.9909515380859375\n",
            "iteration 196/279\n",
            "loss = 4.437966346740723\n",
            "iteration 197/279\n",
            "loss = 4.474909782409668\n",
            "iteration 198/279\n",
            "loss = 5.308760166168213\n",
            "iteration 199/279\n",
            "loss = 8.403435707092285\n",
            "iteration 200/279\n",
            "loss = 4.559223175048828\n",
            "iteration 201/279\n",
            "loss = 6.7235026359558105\n",
            "iteration 202/279\n",
            "loss = 8.252961158752441\n",
            "iteration 203/279\n",
            "loss = 6.217079162597656\n",
            "iteration 204/279\n",
            "loss = 6.064924240112305\n",
            "iteration 205/279\n",
            "loss = 5.5711517333984375\n",
            "iteration 206/279\n",
            "loss = 6.763087272644043\n",
            "iteration 207/279\n",
            "loss = 4.7391815185546875\n",
            "iteration 208/279\n",
            "loss = 3.6512398719787598\n",
            "iteration 209/279\n",
            "loss = 4.651828765869141\n",
            "iteration 210/279\n",
            "loss = 3.865830898284912\n",
            "iteration 211/279\n",
            "loss = 4.517293930053711\n",
            "iteration 212/279\n",
            "loss = 6.695937156677246\n",
            "iteration 213/279\n",
            "loss = 7.3375349044799805\n",
            "iteration 214/279\n",
            "loss = 6.825951099395752\n",
            "iteration 215/279\n",
            "loss = 7.265750885009766\n",
            "iteration 216/279\n",
            "loss = 6.771484375\n",
            "iteration 217/279\n",
            "loss = 5.676116943359375\n",
            "iteration 218/279\n",
            "loss = 5.920943260192871\n",
            "iteration 219/279\n",
            "loss = 5.607724189758301\n",
            "iteration 220/279\n",
            "loss = 6.523736000061035\n",
            "iteration 221/279\n",
            "loss = 6.549360752105713\n",
            "iteration 222/279\n",
            "loss = 4.064314365386963\n",
            "iteration 223/279\n",
            "loss = 4.796567440032959\n",
            "iteration 224/279\n",
            "loss = 4.832403182983398\n",
            "iteration 225/279\n",
            "loss = 9.377131462097168\n",
            "iteration 226/279\n",
            "loss = 3.9146347045898438\n",
            "iteration 227/279\n",
            "loss = 5.607701778411865\n",
            "iteration 228/279\n",
            "loss = 4.277394771575928\n",
            "iteration 229/279\n",
            "loss = 6.085381984710693\n",
            "iteration 230/279\n",
            "loss = 6.872958183288574\n",
            "iteration 231/279\n",
            "loss = 5.899065971374512\n",
            "iteration 232/279\n",
            "loss = 4.902989387512207\n",
            "iteration 233/279\n",
            "loss = 8.012567520141602\n",
            "iteration 234/279\n",
            "loss = 7.114983558654785\n",
            "iteration 235/279\n",
            "loss = 6.724783897399902\n",
            "iteration 236/279\n",
            "loss = 11.682952880859375\n",
            "iteration 237/279\n",
            "loss = 5.360245227813721\n",
            "iteration 238/279\n",
            "loss = 5.826564788818359\n",
            "iteration 239/279\n",
            "loss = 8.219804763793945\n",
            "iteration 240/279\n",
            "loss = 8.295642852783203\n",
            "iteration 241/279\n",
            "loss = 6.75213098526001\n",
            "iteration 242/279\n",
            "loss = 6.464826583862305\n",
            "iteration 243/279\n",
            "loss = 6.1185102462768555\n",
            "iteration 244/279\n",
            "loss = 4.52277135848999\n",
            "iteration 245/279\n",
            "loss = 4.5702362060546875\n",
            "iteration 246/279\n",
            "loss = 4.675535202026367\n",
            "iteration 247/279\n",
            "loss = 7.265242576599121\n",
            "iteration 248/279\n",
            "loss = 4.216367721557617\n",
            "iteration 249/279\n",
            "loss = 3.9768762588500977\n",
            "iteration 250/279\n",
            "loss = 4.032759666442871\n",
            "iteration 251/279\n",
            "loss = 4.984605312347412\n",
            "iteration 252/279\n",
            "loss = 6.446564197540283\n",
            "iteration 253/279\n",
            "loss = 7.554149627685547\n",
            "iteration 254/279\n",
            "loss = 7.890524387359619\n",
            "iteration 255/279\n",
            "loss = 6.746984481811523\n",
            "iteration 256/279\n",
            "loss = 5.036633491516113\n",
            "iteration 257/279\n",
            "loss = 6.506937026977539\n",
            "iteration 258/279\n",
            "loss = 11.205698013305664\n",
            "iteration 259/279\n",
            "loss = 4.983001708984375\n",
            "iteration 260/279\n",
            "loss = 6.136032581329346\n",
            "iteration 261/279\n",
            "loss = 5.579198837280273\n",
            "iteration 262/279\n",
            "loss = 4.903737545013428\n",
            "iteration 263/279\n",
            "loss = 5.355887413024902\n",
            "iteration 264/279\n",
            "loss = 6.484971523284912\n",
            "iteration 265/279\n",
            "loss = 5.577301979064941\n",
            "iteration 266/279\n",
            "loss = 5.436882019042969\n",
            "iteration 267/279\n",
            "loss = 5.118481636047363\n",
            "iteration 268/279\n",
            "loss = 5.832841873168945\n",
            "iteration 269/279\n",
            "loss = 5.314032554626465\n",
            "iteration 270/279\n",
            "loss = 5.5502400398254395\n",
            "iteration 271/279\n",
            "loss = 5.423568248748779\n",
            "iteration 272/279\n",
            "loss = 7.3651041984558105\n",
            "iteration 273/279\n",
            "loss = 4.136932373046875\n",
            "iteration 274/279\n",
            "loss = 3.488431692123413\n",
            "iteration 275/279\n",
            "loss = 4.935520648956299\n",
            "iteration 276/279\n",
            "loss = 4.815764427185059\n",
            "iteration 277/279\n",
            "loss = 5.689767837524414\n",
            "iteration 278/279\n",
            "loss = 4.376612663269043\n",
            "iteration 279/279\n",
            "loss = 2.8937277793884277\n",
            "f1 score: 1.9964904785\n",
            "average precision: 0.85989\n",
            "iteration 1/279\n",
            "loss = 4.8974223136901855\n",
            "iteration 2/279\n",
            "loss = 6.365555286407471\n",
            "iteration 3/279\n",
            "loss = 5.256040573120117\n",
            "iteration 4/279\n",
            "loss = 5.537276268005371\n",
            "iteration 5/279\n",
            "loss = 5.864263534545898\n",
            "iteration 6/279\n",
            "loss = 5.432775020599365\n",
            "iteration 7/279\n",
            "loss = 4.045060157775879\n",
            "iteration 8/279\n",
            "loss = 5.551151275634766\n",
            "iteration 9/279\n",
            "loss = 6.375016212463379\n",
            "iteration 10/279\n",
            "loss = 6.936152458190918\n",
            "iteration 11/279\n",
            "loss = 5.0377349853515625\n",
            "iteration 12/279\n",
            "loss = 4.4830827713012695\n",
            "iteration 13/279\n",
            "loss = 6.870244979858398\n",
            "iteration 14/279\n",
            "loss = 6.599494934082031\n",
            "iteration 15/279\n",
            "loss = 6.358382225036621\n",
            "iteration 16/279\n",
            "loss = 3.707411050796509\n",
            "iteration 17/279\n",
            "loss = 4.864475250244141\n",
            "iteration 18/279\n",
            "loss = 4.394688606262207\n",
            "iteration 19/279\n",
            "loss = 5.0897321701049805\n",
            "iteration 20/279\n",
            "loss = 6.953212261199951\n",
            "iteration 21/279\n",
            "loss = 7.627023696899414\n",
            "iteration 22/279\n",
            "loss = 5.432186126708984\n",
            "iteration 23/279\n",
            "loss = 5.492793083190918\n",
            "iteration 24/279\n",
            "loss = 7.362045764923096\n",
            "iteration 25/279\n",
            "loss = 7.06812047958374\n",
            "iteration 26/279\n",
            "loss = 5.301238536834717\n",
            "iteration 27/279\n",
            "loss = 6.359421730041504\n",
            "iteration 28/279\n",
            "loss = 6.8967084884643555\n",
            "iteration 29/279\n",
            "loss = 7.105302333831787\n",
            "iteration 30/279\n",
            "loss = 5.747260093688965\n",
            "iteration 31/279\n",
            "loss = 4.6147236824035645\n",
            "iteration 32/279\n",
            "loss = 5.086789131164551\n",
            "iteration 33/279\n",
            "loss = 3.894559860229492\n",
            "iteration 34/279\n",
            "loss = 5.555794715881348\n",
            "iteration 35/279\n",
            "loss = 7.604887008666992\n",
            "iteration 36/279\n",
            "loss = 7.557929515838623\n",
            "iteration 37/279\n",
            "loss = 5.431175708770752\n",
            "iteration 38/279\n",
            "loss = 4.16343879699707\n",
            "iteration 39/279\n",
            "loss = 4.54953670501709\n",
            "iteration 40/279\n",
            "loss = 4.947071075439453\n",
            "iteration 41/279\n",
            "loss = 5.143852233886719\n",
            "iteration 42/279\n",
            "loss = 4.073846340179443\n",
            "iteration 43/279\n",
            "loss = 5.934300422668457\n",
            "iteration 44/279\n",
            "loss = 4.6034345626831055\n",
            "iteration 45/279\n",
            "loss = 5.332518577575684\n",
            "iteration 46/279\n",
            "loss = 4.34494686126709\n",
            "iteration 47/279\n",
            "loss = 5.593264579772949\n",
            "iteration 48/279\n",
            "loss = 5.042822360992432\n",
            "iteration 49/279\n",
            "loss = 7.131471633911133\n",
            "iteration 50/279\n",
            "loss = 5.751054763793945\n",
            "iteration 51/279\n",
            "loss = 4.504876136779785\n",
            "iteration 52/279\n",
            "loss = 5.793125152587891\n",
            "iteration 53/279\n",
            "loss = 5.946645736694336\n",
            "iteration 54/279\n",
            "loss = 4.072936058044434\n",
            "iteration 55/279\n",
            "loss = 5.242383003234863\n",
            "iteration 56/279\n",
            "loss = 5.256726264953613\n",
            "iteration 57/279\n",
            "loss = 7.548027992248535\n",
            "iteration 58/279\n",
            "loss = 5.516845703125\n",
            "iteration 59/279\n",
            "loss = 4.168225288391113\n",
            "iteration 60/279\n",
            "loss = 5.820098876953125\n",
            "iteration 61/279\n",
            "loss = 3.2577855587005615\n",
            "iteration 62/279\n",
            "loss = 3.913512706756592\n",
            "iteration 63/279\n",
            "loss = 6.662195205688477\n",
            "iteration 64/279\n",
            "loss = 4.474923610687256\n",
            "iteration 65/279\n",
            "loss = 7.2051496505737305\n",
            "iteration 66/279\n",
            "loss = 5.382855415344238\n",
            "iteration 67/279\n",
            "loss = 5.985321044921875\n",
            "iteration 68/279\n",
            "loss = 8.276649475097656\n",
            "iteration 69/279\n",
            "loss = 5.0296478271484375\n",
            "iteration 70/279\n",
            "loss = 5.256359577178955\n",
            "iteration 71/279\n",
            "loss = 7.246946334838867\n",
            "iteration 72/279\n",
            "loss = 5.906919002532959\n",
            "iteration 73/279\n",
            "loss = 3.9619503021240234\n",
            "iteration 74/279\n",
            "loss = 5.225065231323242\n",
            "iteration 75/279\n",
            "loss = 4.072515487670898\n",
            "iteration 76/279\n",
            "loss = 3.671365737915039\n",
            "iteration 77/279\n",
            "loss = 4.360391139984131\n",
            "iteration 78/279\n",
            "loss = 8.33835220336914\n",
            "iteration 79/279\n",
            "loss = 7.511166095733643\n",
            "iteration 80/279\n",
            "loss = 5.044958591461182\n",
            "iteration 81/279\n",
            "loss = 6.619664192199707\n",
            "iteration 82/279\n",
            "loss = 4.708776950836182\n",
            "iteration 83/279\n",
            "loss = 5.962686538696289\n",
            "iteration 84/279\n",
            "loss = 6.09423828125\n",
            "iteration 85/279\n",
            "loss = 5.106338977813721\n",
            "iteration 86/279\n",
            "loss = 3.8480987548828125\n",
            "iteration 87/279\n",
            "loss = 4.763098239898682\n",
            "iteration 88/279\n",
            "loss = 5.765050888061523\n",
            "iteration 89/279\n",
            "loss = 6.26353645324707\n",
            "iteration 90/279\n",
            "loss = 5.028242111206055\n",
            "iteration 91/279\n",
            "loss = 4.900778770446777\n",
            "iteration 92/279\n",
            "loss = 5.335026741027832\n",
            "iteration 93/279\n",
            "loss = 6.3008036613464355\n",
            "iteration 94/279\n",
            "loss = 3.2958998680114746\n",
            "iteration 95/279\n",
            "loss = 3.5037145614624023\n",
            "iteration 96/279\n",
            "loss = 4.785333633422852\n",
            "iteration 97/279\n",
            "loss = 4.628341197967529\n",
            "iteration 98/279\n",
            "loss = 5.9200334548950195\n",
            "iteration 99/279\n",
            "loss = 5.787291049957275\n",
            "iteration 100/279\n",
            "loss = 4.800487518310547\n",
            "iteration 101/279\n",
            "loss = 7.719725608825684\n",
            "iteration 102/279\n",
            "loss = 6.878611087799072\n",
            "iteration 103/279\n",
            "loss = 4.2063446044921875\n",
            "iteration 104/279\n",
            "loss = 4.812063217163086\n",
            "iteration 105/279\n",
            "loss = 4.631262302398682\n",
            "iteration 106/279\n",
            "loss = 4.279143333435059\n",
            "iteration 107/279\n",
            "loss = 5.0878424644470215\n",
            "iteration 108/279\n",
            "loss = 5.421271324157715\n",
            "iteration 109/279\n",
            "loss = 5.114645957946777\n",
            "iteration 110/279\n",
            "loss = 4.635178565979004\n",
            "iteration 111/279\n",
            "loss = 3.8641443252563477\n",
            "iteration 112/279\n",
            "loss = 5.711954593658447\n",
            "iteration 113/279\n",
            "loss = 4.092073440551758\n",
            "iteration 114/279\n",
            "loss = 5.284078598022461\n",
            "iteration 115/279\n",
            "loss = 5.271924018859863\n",
            "iteration 116/279\n",
            "loss = 5.257525444030762\n",
            "iteration 117/279\n",
            "loss = 2.694619655609131\n",
            "iteration 118/279\n",
            "loss = 3.418182611465454\n",
            "iteration 119/279\n",
            "loss = 7.856285095214844\n",
            "iteration 120/279\n",
            "loss = 8.59786605834961\n",
            "iteration 121/279\n",
            "loss = 5.844437599182129\n",
            "iteration 122/279\n",
            "loss = 11.915220260620117\n",
            "iteration 123/279\n",
            "loss = 5.4644389152526855\n",
            "iteration 124/279\n",
            "loss = 5.253790378570557\n",
            "iteration 125/279\n",
            "loss = 5.846057891845703\n",
            "iteration 126/279\n",
            "loss = 5.716443061828613\n",
            "iteration 127/279\n",
            "loss = 5.448877334594727\n",
            "iteration 128/279\n",
            "loss = 3.847327470779419\n",
            "iteration 129/279\n",
            "loss = 4.039220809936523\n",
            "iteration 130/279\n",
            "loss = 3.508633613586426\n",
            "iteration 131/279\n",
            "loss = 4.473845481872559\n",
            "iteration 132/279\n",
            "loss = 8.328737258911133\n",
            "iteration 133/279\n",
            "loss = 5.4048614501953125\n",
            "iteration 134/279\n",
            "loss = 4.507622241973877\n",
            "iteration 135/279\n",
            "loss = 10.29143238067627\n",
            "iteration 136/279\n",
            "loss = 4.732023239135742\n",
            "iteration 137/279\n",
            "loss = 5.547977447509766\n",
            "iteration 138/279\n",
            "loss = 4.801654815673828\n",
            "iteration 139/279\n",
            "loss = 6.3944292068481445\n",
            "iteration 140/279\n",
            "loss = 6.660170555114746\n",
            "iteration 141/279\n",
            "loss = 4.440093040466309\n",
            "iteration 142/279\n",
            "loss = 4.455550670623779\n",
            "iteration 143/279\n",
            "loss = 5.160384178161621\n",
            "iteration 144/279\n",
            "loss = 9.294322967529297\n",
            "iteration 145/279\n",
            "loss = 6.8036346435546875\n",
            "iteration 146/279\n",
            "loss = 4.909832000732422\n",
            "iteration 147/279\n",
            "loss = 6.312953948974609\n",
            "iteration 148/279\n",
            "loss = 5.998485088348389\n",
            "iteration 149/279\n",
            "loss = 5.301602363586426\n",
            "iteration 150/279\n",
            "loss = 5.155095100402832\n",
            "iteration 151/279\n",
            "loss = 5.005516052246094\n",
            "iteration 152/279\n",
            "loss = 8.426675796508789\n",
            "iteration 153/279\n",
            "loss = 5.889947414398193\n",
            "iteration 154/279\n",
            "loss = 4.414602279663086\n",
            "iteration 155/279\n",
            "loss = 6.841461181640625\n",
            "iteration 156/279\n",
            "loss = 6.1087188720703125\n",
            "iteration 157/279\n",
            "loss = 5.979581832885742\n",
            "iteration 158/279\n",
            "loss = 6.561494827270508\n",
            "iteration 159/279\n",
            "loss = 6.021449089050293\n",
            "iteration 160/279\n",
            "loss = 5.953921794891357\n",
            "iteration 161/279\n",
            "loss = 5.542571544647217\n",
            "iteration 162/279\n",
            "loss = 5.991294860839844\n",
            "iteration 163/279\n",
            "loss = 11.580810546875\n",
            "iteration 164/279\n",
            "loss = 5.430926322937012\n",
            "iteration 165/279\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4a3b6448544a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Track loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'iteration {iteration+1}/{n_iter}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'loss = {loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_history = []\n",
        "f1_history = []\n",
        "avg_prec_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    net.train()\n",
        "    for iteration, (train_inputs, train_targets) in enumerate(train_dataloader):\n",
        "        net.train()\n",
        "        # Unpack targets and cast to float\n",
        "        start_logits, end_logits = train_targets\n",
        "        start_logits, end_logits = torch.tensor(start_logits, dtype=torch.float32, device=device), torch.tensor(end_logits, dtype=torch.float32, device=device)\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        start_out, end_out = net.forward(train_inputs)\n",
        "        # Loss function\n",
        "        ## TOCHECK\n",
        "        loss = loss_fn(start_out, start_logits) + loss_fn(end_out, end_logits)\n",
        "        # Gradient update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "#\n",
        "        # Track loss\n",
        "        print(f'iteration {iteration+1}/{n_iter}')\n",
        "        print(f'loss = {loss}')\n",
        "        loss_history.append(loss)\n",
        "    \n",
        "    #if epoch % 5 == 0:\n",
        "    net.eval()\n",
        "    val_inputs, val_targets = next(val_dataloader)\n",
        "    f1, avg_prec = evaluate(net, val_inputs, val_targets)\n",
        "    f1_history.append(f1)\n",
        "    avg_prec_history.append(avg_prec)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OK4tFfSgF2c3"
      },
      "id": "OK4tFfSgF2c3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "QA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}