{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c69e289",
   "metadata": {
    "id": "3c69e289"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291d7474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:05.154416Z",
     "start_time": "2022-01-12T12:45:57.538962Z"
    },
    "id": "291d7474"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Suppress output\n",
    "\n",
    "# Whether the notebook is run within Google Colab or not\n",
    "colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# Install needed dependencies on Colab\n",
    "if colab:\n",
    "    !pip install transformers\n",
    "from transformers import DistilBertModel#, DistilBertTokenizerFast\n",
    "\n",
    "# Enable GPU acceleration, whenever available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Automatically reimport modules at each execution\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86992ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:05.223020Z",
     "start_time": "2022-01-12T12:46:05.157398Z"
    }
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !git clone 'https://github.com/michimichiamo/question-answering'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa58304",
   "metadata": {
    "id": "eaa58304"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3124ea1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:05.281868Z",
     "start_time": "2022-01-12T12:46:05.226393Z"
    },
    "id": "3124ea1c"
   },
   "outputs": [],
   "source": [
    "# Execute this only to load the dataset in csv format if not already done\n",
    "# from read_dataset import read_dataset\n",
    "\n",
    "# dataset = read_dataset(path='training_set.json', validation_set_perc=20)\n",
    "# train_df = pd.DataFrame(dataset[0], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
    "# train_df.to_csv('train_df.csv')\n",
    "# val_df = pd.DataFrame(dataset[1], columns=['id', 'title', 'context_id', 'context', 'question', 'start', 'end'])\n",
    "# val_df.to_csv('val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb334c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:05.344966Z",
     "start_time": "2022-01-12T12:46:05.284455Z"
    },
    "id": "8a453903"
   },
   "outputs": [],
   "source": [
    "directory='./' if not colab else './question-answering/'\n",
    "\n",
    "train_filename = directory+'data/tokenized/train.npz'\n",
    "#val_filename = directory+'data/tokenized/val.npz'\n",
    "\n",
    "train_data = np.load(train_filename)\n",
    "#val_data = np.load(val_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b01bd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.116897Z",
     "start_time": "2022-01-12T12:46:05.347475Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = train_data['input_ids']\n",
    "attention_mask = train_data['attention_mask']\n",
    "answer_start = train_data['answer_start']\n",
    "answer_end = train_data['answer_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef63b625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.180318Z",
     "start_time": "2022-01-12T12:46:07.119253Z"
    },
    "id": "8a453903"
   },
   "outputs": [],
   "source": [
    "#train_df = pd.DataFrame()\n",
    "#train_df['input_ids'] = [i for i in train_data['input_ids']]\n",
    "#train_df['attention_mask'] = [i for i in train_data['attention_mask']]\n",
    "#train_df['answer_start'] = train_data['answer_start']\n",
    "#train_df['answer_end'] = train_data['answer_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a453903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.241819Z",
     "start_time": "2022-01-12T12:46:07.184972Z"
    },
    "id": "8a453903"
   },
   "outputs": [],
   "source": [
    "#val_df = pd.DataFrame()\n",
    "#val_df['input_ids'] = [i for i in val_data['input_ids']]\n",
    "#val_df['attention_mask'] = [i for i in val_data['attention_mask']]\n",
    "#val_df['answer_start'] = val_data['answer_start']\n",
    "#val_df['answer_end'] = val_data['answer_end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6f886",
   "metadata": {
    "id": "0bd6f886"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862509a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.300283Z",
     "start_time": "2022-01-12T12:46:07.245503Z"
    },
    "id": "862509a4"
   },
   "outputs": [],
   "source": [
    "## Load tokenizer and transformers\n",
    "\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "#model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0fdafe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.359705Z",
     "start_time": "2022-01-12T12:46:07.302745Z"
    },
    "cellView": "code",
    "code_folding": [],
    "id": "9a0fdafe"
   },
   "outputs": [],
   "source": [
    "## Tokenize questions and contexts\n",
    "#max_length = model.config.max_position_embeddings\n",
    "#doc_stride = 256\n",
    "#\n",
    "#train_tokenized = tokenizer(\n",
    "#   train_questions,\n",
    "#   train_contexts,\n",
    "#   max_length=max_length,\n",
    "#   truncation=\"only_second\",\n",
    "#   return_overflowing_tokens=True,\n",
    "#   return_offsets_mapping=True,\n",
    "#   stride=doc_stride,\n",
    "#   return_attention_mask=True,\n",
    "#   padding='max_length'\n",
    "#)\n",
    "#\n",
    "#val_tokenized = tokenizer(\n",
    "#   val_questions,\n",
    "#   val_contexts,\n",
    "#   max_length=max_length,\n",
    "#   truncation=\"only_second\",\n",
    "#   return_overflowing_tokens=True,\n",
    "#   return_offsets_mapping=True,\n",
    "#   stride=doc_stride,\n",
    "#   return_attention_mask=True,\n",
    "#   padding='max_length'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0Bc1VhcCiQF",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.416560Z",
     "start_time": "2022-01-12T12:46:07.362296Z"
    },
    "id": "f0Bc1VhcCiQF"
   },
   "outputs": [],
   "source": [
    "## Move to device\n",
    "#bert_dict = {}\n",
    "#\n",
    "#bert_dict['input_ids'] = torch.IntTensor(tokenized['input_ids']).to(device)\n",
    "#bert_dict['attention_mask'] = torch.IntTensor(tokenized['attention_mask']).to(device)\n",
    "#\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ldtj3wxmCU9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.474199Z",
     "start_time": "2022-01-12T12:46:07.419464Z"
    },
    "id": "ldtj3wxmCU9b"
   },
   "outputs": [],
   "source": [
    "## Network structure\n",
    "\n",
    "#transformed = model(**bert_dict)\n",
    "#dropped = torch.nn.Dropout(0.3)(transformed[0])\n",
    "#logits = torch.nn.Linear(768, 2, device=device)(dropped)\n",
    "#start_logits, end_logits = logits.split(1, dim=-1)\n",
    "#start_logits = start_logits.squeeze(-1)\n",
    "#end_logits = end_logits.squeeze(-1)\n",
    "#outputs = (start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd5836ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.533414Z",
     "start_time": "2022-01-12T12:46:07.477094Z"
    },
    "id": "bd5836ff"
   },
   "outputs": [],
   "source": [
    "#tokenizer.decode(tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04a525d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.590646Z",
     "start_time": "2022-01-12T12:46:07.536268Z"
    },
    "id": "04a525d5"
   },
   "outputs": [],
   "source": [
    "#train_df['question'].apply(lambda x: len(x.strip().split(' '))).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7QB6LO9WCpyO",
   "metadata": {
    "id": "7QB6LO9WCpyO"
   },
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4d22e",
   "metadata": {
    "id": "22c4d22e"
   },
   "source": [
    "- `tokenized['offset_mapping'][0]` returna le tuple (start,end) di ogni parola dell'input (query, context)\n",
    "\n",
    "- Problema: splittare i contesti online (nel Dataloader) produce batch di lunghezza variabile\n",
    "    - Prima soluzione: eliminare gli split che non contengono la domanda\n",
    "    - Seconda soluzione: creare dataframe con contesti gi√† splittati usando il tokenizer (lunghezza 512, overlapping 256) invece che farlo online nel Dataloader\n",
    "\n",
    "- Problema: risposte sono presenti in un solo split, cosa fare con gli altri?\n",
    "    - Una [soluzione](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=iLekL6Un9D70&line=24&uniqifier=1): riscalare tuple di contesti tagliati (invece che 0, allinearli alla risposta)\n",
    "\n",
    "    - Un'altra soluzione: \n",
    "        - riscalare answer_start e answer_end per ogni contesto\n",
    "        - lo split che contiene la risposta mantiene answer_start e answer_end, gli altri split dello stesso contesto vanno trattati (assegniamo (0,0)? oppure scartiamo)\n",
    "        - Possibile [soluzione](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=v86c_RApFdNG) (in grassetto parte interessante): *Now let's put everything together in one function we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), **we set the cls index for both the start and end position**. We could also simply discard those examples from the training set if the flag allow_impossible_answers is False. Since the preprocessing is already complex enough as it is, we've kept is simple for this part.*\n",
    "\n",
    "- N.B. sul token `[CLS]` preso da [qui](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=kv1iD9E6FdND):\n",
    "> *The very first token ([CLS]) has (0, 0) because it doesn't correspond to any part of the question/answer, then the second token is the same as the characters 0 to 3 of the question*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205600e",
   "metadata": {
    "id": "6205600e"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4916e6fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:07.652998Z",
     "start_time": "2022-01-12T12:46:07.593116Z"
    },
    "id": "4916e6fc"
   },
   "outputs": [],
   "source": [
    "class QA(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=768, num_labels=2, dropout_rate=0.5):\n",
    "        super(QA, self).__init__()\n",
    "        # Device\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Layers\n",
    "        #self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "        self.transformers = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad').to(self.device)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        #self.extra_linear = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        #self.extra_linear_tanh = torch.nn.Tanh()\n",
    "        self.dense = torch.nn.Linear(self.hidden_size, self.num_labels, device=self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Unpack inputs\n",
    "        input_ids, attention_mask = inputs\n",
    "        \n",
    "        # Put to device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        \n",
    "        # Transformers \n",
    "        transformed = self.transformers(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Dropout\n",
    "        dropped = self.dropout(transformed[0])\n",
    "        # Obtain logits\n",
    "        logits = self.dense(dropped) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
    "        start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
    "        end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
    "        # --- 4) Prepare output tuple\n",
    "        outputs = (start_logits, end_logits)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cFqtEIvq9bio",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:10.796206Z",
     "start_time": "2022-01-12T12:46:07.655319Z"
    },
    "id": "cFqtEIvq9bio"
   },
   "outputs": [],
   "source": [
    "net = QA()\n",
    "net = net.to(net.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ZX7lp9lGApN4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:10.864157Z",
     "start_time": "2022-01-12T12:46:10.798299Z"
    },
    "id": "ZX7lp9lGApN4"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, input_ids, attention_masks, answer_starts, answer_ends):\n",
    "        'Initialization'\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.answer_starts = answer_starts\n",
    "        self.answer_ends = answer_ends\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        input_id = self.input_ids[index]\n",
    "        attention_mask = self.attention_masks[index]\n",
    "        answer_start = self.answer_starts[index]\n",
    "        answer_end = self.answer_ends[index]\n",
    "\n",
    "        # Pack input and output\n",
    "        X = (input_id, attention_mask)\n",
    "        y = (answer_start, answer_end)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "QqhlR1A9BKsa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:10.924544Z",
     "start_time": "2022-01-12T12:46:10.866537Z"
    },
    "id": "QqhlR1A9BKsa"
   },
   "outputs": [],
   "source": [
    "data = Dataset(input_ids, attention_mask, answer_start, answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bPt21jd5-ZD9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:10.983614Z",
     "start_time": "2022-01-12T12:46:10.927066Z"
    },
    "id": "bPt21jd5-ZD9"
   },
   "outputs": [],
   "source": [
    "generator = torch.utils.data.DataLoader(data, batch_size=32)#, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bea5699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T12:46:11.065721Z",
     "start_time": "2022-01-12T12:46:10.986537Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dhvSBZ549wLG",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T14:39:15.485889Z",
     "start_time": "2022-01-12T12:46:11.068018Z"
    },
    "id": "dhvSBZ549wLG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "iterations=0\n",
    "\n",
    "for inputs, targets in generator:\n",
    "    # Unpack targets\n",
    "    start_tgt, end_tgt = targets\n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    start_out, end_out = net.forward(inputs)\n",
    "    # Convert targets to probabilities\n",
    "    start_logits, end_logits = torch.zeros(size=start_out.size(), dtype=torch.float64),  torch.zeros(size=end_out.size(), dtype=torch.float64)\n",
    "    for idx, (start, end) in enumerate(zip(start_logits, end_logits)):\n",
    "        start[start_tgt[idx]] = 1\n",
    "        end[end_tgt[idx]] = 1\n",
    "    # Loss function\n",
    "    loss = loss_fn(start_logits, start_out) + loss_fn(end_logits, end_out)\n",
    "    # Gradient update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    iterations += 1\n",
    "    print(iterations)\n",
    "    if iterations==100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81285af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "QA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
