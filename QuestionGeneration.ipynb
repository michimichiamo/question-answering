{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and imports"
      ],
      "metadata": {
        "id": "04qUhbikgSkc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbwl6E1E205R"
      },
      "source": [
        "%%capture\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install rich[jupyter]\n",
        "!pip install torchmetrics==0.6"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console"
      ],
      "metadata": {
        "id": "MpLxzQyNgQfs"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -r question-answering\n",
        "!git clone https://github.com/michimichiamo/question-answering/\n",
        "%cd question-answering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0_XqNPyc8hJ",
        "outputId": "d9a13c20-124f-4a30-90f0-824e54fe93dc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'question-answering'...\n",
            "remote: Enumerating objects: 318, done.\u001b[K\n",
            "remote: Counting objects: 100% (305/305), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 318 (delta 161), reused 159 (delta 59), pack-reused 13\u001b[K\n",
            "Receiving objects: 100% (318/318), 115.84 MiB | 11.98 MiB/s, done.\n",
            "Resolving deltas: 100% (164/164), done.\n",
            "Checking out files: 100% (29/29), done.\n",
            "/content/question-answering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from util.model import read_npz\n",
        "\n",
        "tr_ids, tr_contexts, tr_attention_masks, tr_questions = read_npz(path='./data/tokenized-qg/', split='train', task='QG')\n",
        "val_ids, val_contexts, val_attention_masks, val_questions = read_npz(path='./data/tokenized-qg/', split='val', task='QG')"
      ],
      "metadata": {
        "id": "0pEj0LowdM_-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB441x104K-o"
      },
      "source": [
        "# define a rich console logger\n",
        "console=Console(record=True)\n",
        "\n",
        "def display_df(df):\n",
        "    \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "    console=Console()\n",
        "    table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
        "\n",
        "    for i, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
        "                        Column(\"Steps\", justify=\"center\"),\n",
        "                        Column(\"Loss\", justify=\"center\"), \n",
        "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "source": [
        "#class YourDataSetClass(torch.utils.data.Dataset):\n",
        "#  \"\"\"\n",
        "#  Creating a custom dataset for reading the dataset and \n",
        "#  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "#\n",
        "#  \"\"\"\n",
        "#\n",
        "#  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "#    self.tokenizer = tokenizer\n",
        "#    self.data = dataframe\n",
        "#    self.source_len = source_len\n",
        "#    self.summ_len = target_len\n",
        "#    self.target_text = self.data[target_text]\n",
        "#    self.source_text = self.data[source_text]\n",
        "#\n",
        "#  def __len__(self):\n",
        "#    return len(self.target_text)\n",
        "#\n",
        "#  def __getitem__(self, index):\n",
        "#    source_text = str(self.source_text[index])\n",
        "#    target_text = str(self.target_text[index])\n",
        "#\n",
        "#    #cleaning data so as to ensure data is in string type\n",
        "#    source_text = ' '.join(source_text.split())\n",
        "#    target_text = ' '.join(target_text.split())\n",
        "#\n",
        "#    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "#    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "#\n",
        "#    source_ids = source['input_ids'].squeeze()\n",
        "#    source_mask = source['attention_mask'].squeeze()\n",
        "#    target_ids = target['input_ids'].squeeze()\n",
        "#    target_mask = target['attention_mask'].squeeze()\n",
        "#\n",
        "#    return {\n",
        "#        'source_ids': source_ids.to(dtype=torch.long), \n",
        "#        'source_mask': source_mask.to(dtype=torch.long), \n",
        "#        'target_ids': target_ids.to(dtype=torch.long),\n",
        "#        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "#    }"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, ids, contexts, attention_masks, questions):\n",
        "        'Initialization'\n",
        "        self.ids = ids\n",
        "        self.contexts = contexts\n",
        "        self.attention_masks = attention_masks\n",
        "        self.questions = questions\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.ids[index]\n",
        "        context = torch.tensor(self.contexts[index], dtype=torch.int32)\n",
        "        attention_mask = torch.tensor(self.attention_masks[index], dtype=torch.int32)\n",
        "        question = torch.tensor(self.questions[index], dtype=torch.int32)\n",
        "\n",
        "        # Pack input and output\n",
        "        X = (ID, context, attention_mask)\n",
        "        y = question\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "hPnGyF5Fdxpk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkj6wIMt40RK"
      },
      "source": [
        "def train(model, optimizer, epoch, loader, pad_token, device):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    loss_history = []\n",
        "    for iteration, (X,y) in enumerate(loader, 0):\n",
        "        # Unpack input\n",
        "        _, context, attention_mask = X\n",
        "\n",
        "        # Prepare input\n",
        "        context = context.to(device, dtype = torch.long)\n",
        "        attention_mask = attention_mask.to(device, dtype = torch.long)\n",
        "\n",
        "        # Prepare target\n",
        "        question = y.to(device, dtype = torch.long)\n",
        "        question_ids = question.contiguous()\n",
        "        lm_labels = question.clone().detach()\n",
        "        lm_labels[question == pad_token] = -100\n",
        "\n",
        "        outputs = model(input_ids = context, attention_mask = attention_mask,\n",
        "                        decoder_input_ids=question_ids, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if iteration%10==0:\n",
        "            training_logger.add_row(str(epoch), str(iteration), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_history.append(loss)\n",
        "    return loss_history"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUBykK-A43DF"
      },
      "source": [
        "def validate(model, epoch, loader, tokenizer, device):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to evaluate model for predictions\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for iteration, (X,y) in enumerate(loader, 0):\n",
        "            # Unpack input\n",
        "            _, context, attention_mask = X\n",
        "\n",
        "            question = y.to(device, dtype = torch.long)\n",
        "            context = context.to(device, dtype = torch.long)\n",
        "            attention_mask = attention_mask.to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = context,\n",
        "                attention_mask = attention_mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in question]\n",
        "            if iteration%10==0:\n",
        "                console.print(f'Completed {iteration}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(model_params):\n",
        "    \"\"\"\n",
        "    Model definition\n",
        "\n",
        "    \"\"\"\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
        "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # logging\n",
        "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # tokenizer for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"TOKENIZER\"])\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "    return model, optimizer, tokenizer\n",
        "\n",
        "def prepare_data(train_dataset, val_dataset, model_params):\n",
        "    # logging\n",
        "    console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0,\n",
        "        'pin_memory' : True\n",
        "        }\n",
        "\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "        'shuffle': False,\n",
        "        'num_workers': 0,\n",
        "        'pin_memory' : True\n",
        "        }\n",
        "\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, **train_params)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, **val_params)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def training_loop(model, optimizer, tokenizer, model_params,\n",
        "                  train_loader, val_loader, save=False, output_dir=\"./outputs/\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Model training\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Training loop\n",
        "    console.log(f'[Initiating Fine Tuning]...\\n')\n",
        "\n",
        "    pad_token = tokenizer.pad_token_id\n",
        "    loss_history = []\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        loss = train(model, optimizer, epoch, train_loader, pad_token, device)\n",
        "        loss_history += loss\n",
        "    \n",
        "    #Saving the model after training    \n",
        "    if save:\n",
        "        console.log(f\"[Saving Model]...\\n\")\n",
        "        path = os.path.join(output_dir, \"model_files\")\n",
        "        model.save_pretrained(path)\n",
        "        tokenizer.save_pretrained(path)\n",
        "        np.save(\"./outputs/loss_history.npy\", np.array(loss_history))\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    console.log(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(model, epoch, val_loader, tokenizer, device)\n",
        "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "  "
      ],
      "metadata": {
        "id": "CUrfp1XMa8uC"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxCpQwD8PDIs"
      },
      "source": [
        "ours = False\n",
        "model_params={\n",
        "    \"MODEL\":\"t5-small\" if not ours else './model',            # model_type: t5-base/t5-large\n",
        "    \"TOKENIZER\": \"t5-small\",\n",
        "    \"TRAIN_BATCH_SIZE\":8,          # training batch size\n",
        "    \"VALID_BATCH_SIZE\":8,          # validation batch size\n",
        "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
        "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
        "    \"LEARNING_RATE\":1e-5,          # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\":50,   # max length of target text\n",
        "    \"SEED\": 42                     # set seed for reproducibility \n",
        "\n",
        "}"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_sz = -1\n",
        "val_sz = -1\n",
        "\n",
        "train_dataset = Dataset(tr_ids[:tr_sz], tr_contexts[:tr_sz], tr_attention_masks[:tr_sz], tr_questions[:tr_sz])\n",
        "val_dataset = Dataset(val_ids[:val_sz], val_contexts[:val_sz], val_attention_masks[:val_sz], val_questions[:val_sz])"
      ],
      "metadata": {
        "id": "vEjxV0lTf7uw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, tokenizer = define_model(model_params)\n",
        "train_loader, val_loader = prepare_data(train_dataset, val_dataset, model_params)"
      ],
      "metadata": {
        "id": "HgHbeDMbefu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "1a2dc3b9-ccc2-4c81-852c-64659c128b96"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:33:38] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-small<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-70-d6335b0fceca&gt;:12</span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[15:33:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-small\u001b[33m...\u001b[0m                    \u001b[2m<ipython-input-70-d6335b0fceca>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m12\u001b[0m\n",
              "\u001b[2;36m           \u001b[0m                                                \u001b[2m                                  \u001b[0m\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:33:44] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-70-d6335b0fceca&gt;:29</span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[15:33:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                         \u001b[2m<ipython-input-70-d6335b0fceca>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m29\u001b[0m\n",
              "\u001b[2;36m           \u001b[0m                                                \u001b[2m                                  \u001b[0m\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del train_dataset, val_dataset\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "MhudLTkohO3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89607ae0-236f-4cd2-a886-92ece4d1ec3a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "332"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(model, optimizer, tokenizer, model_params,\n",
        "            train_loader, val_loader, save=True, output_dir=\"./outputs/\")"
      ],
      "metadata": {
        "id": "EGZuEToreqLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "ac87e2fd-d5b0-46e0-8be2-e7dd048f26e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(8.6072, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  10   | tensor(8.2856, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  20   | tensor(7.6444, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  30   | tensor(7.3082, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  40   | tensor(7.4560, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  50   | tensor(6.6037, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  60   | tensor(7.2990, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  70   | tensor(6.9285, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  80   | tensor(7.1322, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  90   | tensor(6.9268, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(8.6072, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  10   | tensor(8.2856, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  20   | tensor(7.6444, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  30   | tensor(7.3082, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  40   | tensor(7.4560, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  50   | tensor(6.6037, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  60   | tensor(7.2990, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  70   | tensor(6.9285, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  80   | tensor(7.1322, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  90   | tensor(6.9268, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(8.6072, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  10   | tensor(8.2856, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  20   | tensor(7.6444, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  30   | tensor(7.3082, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  40   | tensor(7.4560, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  50   | tensor(6.6037, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  60   | tensor(7.2990, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  70   | tensor(6.9285, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  80   | tensor(7.1322, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  90   | tensor(6.9268, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  100  | tensor(6.8468, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(8.6072, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  10   | tensor(8.2856, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  20   | tensor(7.6444, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  30   | tensor(7.3082, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  40   | tensor(7.4560, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  50   | tensor(6.6037, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  60   | tensor(7.2990, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  70   | tensor(6.9285, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  80   | tensor(7.1322, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  90   | tensor(6.9268, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  100  | tensor(6.8468, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(8.6072, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  10   | tensor(8.2856, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  20   | tensor(7.6444, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  30   | tensor(7.3082, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  40   | tensor(7.4560, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  50   | tensor(6.6037, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  60   | tensor(7.2990, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  70   | tensor(6.9285, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  80   | tensor(7.1322, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  90   | tensor(6.9268, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  100  | tensor(6.8468, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   |  110  | tensor(6.5657, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(8.6072, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  10   | tensor(8.2856, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  20   | tensor(7.6444, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  30   | tensor(7.3082, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  40   | tensor(7.4560, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  50   | tensor(6.6037, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  60   | tensor(7.2990, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  70   | tensor(6.9285, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  80   | tensor(7.1322, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  90   | tensor(6.9268, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  100  | tensor(6.8468, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   |  110  | tensor(6.5657, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save = False\n",
        "output_dir=\"./outputs/\""
      ],
      "metadata": {
        "id": "l6l6wf5Chjb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2qL87Wsn19"
      },
      "source": [
        "\"\"\"\n",
        "Model training\n",
        "\"\"\"\n",
        "# Training loop\n",
        "console.log(f'[Initiating Fine Tuning]...\\n')\n",
        "for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "    train(epoch, tokenizer, model, device, train_loader, optimizer)\n",
        "\n",
        "#Saving the model after training    \n",
        "if save:\n",
        "    console.log(f\"[Saving Model]...\\n\")\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "# Evaluation\n",
        "console.log(f\"[Initiating Validation]...\\n\")\n",
        "for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "\n",
        "console.save_text(os.path.join(output_dir,'logs.txt'))\n",
        "\n",
        "console.log(f\"[Validation Completed.]\\n\")\n",
        "console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
        "console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
        "console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_s = 0\n",
        "tr_sz = 8\n",
        "train_dataset = Dataset(tr_ids[tr_s:tr_sz], tr_contexts[tr_s:tr_sz], tr_attention_masks[tr_s:tr_sz], tr_questions[tr_s:tr_sz])\n",
        "\n",
        "val_dataset = Dataset(val_ids[:8], val_contexts[:8], val_attention_masks[:8], val_questions[:8])"
      ],
      "metadata": {
        "id": "HSs3Wn9Iy8Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader = prepare_data(train_dataset, val_dataset, model_params)"
      ],
      "metadata": {
        "id": "Mwoj21-dzMjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unvWOxvkJUsR"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('./model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration, (X,y) in enumerate(val_loader, 0):\n",
        "  # Unpack input\n",
        "  _, context, attention_mask = X\n",
        "\n",
        "  question = y.to(device, dtype = torch.long)\n",
        "  context = context.to(device, dtype = torch.long)\n",
        "  attention_mask = attention_mask.to(device, dtype = torch.long)\n",
        "\n",
        "  generated_ids = model.generate(\n",
        "    input_ids = context,\n",
        "    attention_mask = attention_mask, \n",
        "    max_length=150, \n",
        "    num_beams=2,\n",
        "    repetition_penalty=1.5, \n",
        "    length_penalty=1.5, \n",
        "    early_stopping=True,\n",
        "    do_sample=False,\n",
        "    )\n",
        "  \n",
        "  context_t = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in context]\n",
        "  preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "  target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in question]\n",
        "  \n",
        "  print(context_t)\n",
        "  print(preds)\n",
        "  print(target)\n",
        "  "
      ],
      "metadata": {
        "id": "zzTWpG1_zZ9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in target:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "6vB0G8VV24LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in preds:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "SiPz_jxJ21vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in context_t:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "IcgdRBcp0700"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dhS8Pi6WVMje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mpsigH3BVa3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}